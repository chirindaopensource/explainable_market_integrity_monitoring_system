{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJDXsM262VTN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# AIMM-X: An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2601.15304v1-b31b1b.svg)](https://arxiv.org/abs/2601.15304v1)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ArXiv%20Preprint-003366)](https://arxiv.org/abs/2601.15304v1)\n",
        "[![Year](https://img.shields.io/badge/Year-2026-purple)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Market%20Microstructure%20%7C%20RegTech-00529B)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Polygon.io%20%7C%20Reddit%20%7C%20Wikipedia-lightgrey)](https://polygon.io/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Hysteresis%20Segmentation-orange)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Factor%20Decomposition-red)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Retrospective%20Case%20Studies-green)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Rolling%20Baseline%20Z--Scores-yellow)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![YAML](https://img.shields.io/badge/YAML-%23CB171E.svg?style=flat&logo=yaml&logoColor=white)](https://yaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![Open Source](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen)](https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system`\n",
        "\n",
        "**Owner:** 2026 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2026 paper entitled **\"An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring\"** by:\n",
        "\n",
        "*   **Sandeep Neela** (Independent Researcher)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from the ingestion and rigorous validation of market microstructure and attention data to the detection of suspicious trading windows via hysteresis segmentation, culminating in the generation of interpretable integrity scores and factor attributions.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_aimm_x_pipeline`](#key-callable-run_aimm_x_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Neela (2026). The core of this repository is the iPython Notebook `explainable_market_integrity_monitoring_system_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline addresses the critical challenge of **market integrity monitoring** by moving away from opaque, proprietary \"black-box\" models toward a transparent, auditable \"glass-box\" approach.\n",
        "\n",
        "The paper argues that effective surveillance requires explainability—analysts must understand *why* a window was flagged—and accessibility to public data sources. This codebase operationalizes the proposed solution: **AIMM-X**, a system that:\n",
        "-   **Validates** data integrity using strict OHLC consistency checks ($H_t \\ge \\max(O_t, C_t)$) and precise missingness semantics (NaN vs. 0).\n",
        "-   **Fuses** multi-source attention signals (Reddit, StockTwits, News, Wikipedia, Google Trends) into a unified metric of public interest.\n",
        "-   **Detects** anomalies using a robust **Hysteresis State Machine** (Schmitt Trigger) that prevents alert fragmentation.\n",
        "-   **Scores** windows using a linear **Integrity Score ($M$)** decomposed into six interpretable factors ($\\phi_1 \\dots \\phi_6$), enabling clear attribution of alerts to price shocks, volatility anomalies, or attention spikes.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Financial Econometrics, Signal Processing, and Explainable AI.\n",
        "\n",
        "**1. Multi-Source Attention Fusion ($A_{i,t}$):**\n",
        "A unified attention signal is constructed by aggregating normalized proxies from diverse sources, capturing the \"hype\" dimension of market activity.\n",
        "$$ A_{i,t} = \\sum_{s \\in \\mathcal{S}} w_s \\cdot \\tilde{a}_{s,i,t} $$\n",
        "where $\\tilde{a}_{s,i,t}$ represents the rolling z-score of source $s$ for ticker $i$ at time $t$.\n",
        "\n",
        "**2. Statistical Deviation Detection:**\n",
        "The system employs dynamic baselines to adapt to changing market regimes, computing standardized deviations (z-scores) for returns ($r$), volatility ($\\sigma$), and attention ($A$).\n",
        "$$ z_{i,t}^{(x)} = \\frac{x_{i,t} - \\mu_{i,t}^{(x)}}{\\hat{\\sigma}_{i,t}^{(x)} + \\epsilon} $$\n",
        "A composite strength score $s_{i,t}$ aggregates these deviations to drive detection.\n",
        "\n",
        "**3. Hysteresis-Based Segmentation:**\n",
        "To avoid \"chattering\" (rapid on/off switching of alerts due to noise), the system uses dual-threshold hysteresis logic:\n",
        "-   **Trigger:** A window opens when $s_{i,t} > \\theta_{\\text{high}}$.\n",
        "-   **Sustain:** A window remains open while $s_{i,t} > \\theta_{\\text{low}}$.\n",
        "-   **Exit:** A window closes only after $s_{i,t} \\le \\theta_{\\text{low}}$ for a specified gap tolerance $g$.\n",
        "\n",
        "**4. Interpretable Integrity Score ($M$):**\n",
        "Detected windows are ranked by a score $M(w)$ that is fully decomposable into additive evidence factors:\n",
        "$$ M(w) = \\sum_{k=1}^{6} \\omega_k \\cdot \\phi_k(w) $$\n",
        "Factors include Return Shock Intensity ($\\phi_1$), Volatility Anomaly ($\\phi_2$), Attention Spike Magnitude ($\\phi_3$), and Co-movement Alignment ($\\phi_4$).\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`explainable_market_integrity_monitoring_system_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The pipeline is decomposed into 17 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters (thresholds, weights, lookback windows) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, OHLC consistency, and exchange calendar alignment.\n",
        "-   **Deterministic Execution:** Enforces reproducibility through seed control, deterministic sorting, and rigorous logging of all stochastic outputs.\n",
        "-   **Comprehensive Audit Logging:** Generates detailed logs of every processing step, including quarantine counts and filter statistics.\n",
        "-   **Reproducible Artifacts:** Generates structured `PipelineResult` objects containing raw window lists, filtered top-N tables, and factor summary statistics.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Configuration & Validation (Task 1):** Loads and validates the study configuration, enforcing parameter constraints and determinism requirements.\n",
        "2.  **Data Ingestion & Cleansing (Tasks 2-3):** Validates panel schema, enforces OHLC consistency, and strictly handles missingness semantics (NaN vs 0).\n",
        "3.  **Calendar Enforcement (Task 4):** Aligns data to the canonical NYSE/Nasdaq trading session grid.\n",
        "4.  **Attention Processing (Tasks 5-7):** Aligns, normalizes, and fuses multi-source attention signals into a unified metric.\n",
        "5.  **Feature Engineering (Tasks 8-9):** Computes log returns and rolling realized volatility proxies.\n",
        "6.  **Deviation Detection (Tasks 10-11):** Computes rolling baselines, z-scores, and the composite strength score.\n",
        "7.  **Window Segmentation (Task 12):** Applies the hysteresis state machine to detect suspicious time intervals.\n",
        "8.  **Scoring & Attribution (Tasks 13-14):** Computes $\\phi$-factors and the composite Integrity Score $M$ with full decomposition.\n",
        "9.  **Ranking & Filtering (Task 15):** Ranks windows by score and applies warmup/artifact filters.\n",
        "10. **Artifact Generation (Task 16):** Produces final output tables and summary statistics.\n",
        "11. **Orchestration (Task 17):** Unifies all components into a single `run_aimm_x_pipeline` function.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The notebook is structured as a logical pipeline with modular orchestrator functions for each of the 17 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_aimm_x_pipeline`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_aimm_x_pipeline`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between validation, cleansing, detection, scoring, and reporting modules.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `pyyaml`.\n",
        "-   Optional dependencies: `exchange_calendars` (for precise trading session generation).\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system.git\n",
        "    cd explainable_market_integrity_monitoring_system\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy pyyaml exchange_calendars\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a primary DataFrame `df_raw_panel` with a MultiIndex `(date, ticker)` and the following columns:\n",
        "\n",
        "**Market Microstructure:**\n",
        "1.  **`open_price`**: Float.\n",
        "2.  **`high_price`**: Float, $\\ge \\max(Open, Close)$.\n",
        "3.  **`low_price`**: Float, $\\le \\min(Open, Close)$.\n",
        "4.  **`close_price`**: Float, $>0$.\n",
        "5.  **`volume`**: Float/Int, $>0$.\n",
        "\n",
        "**Attention Signals (Nullable):**\n",
        "1.  **`reddit_posts`**: Float (count).\n",
        "2.  **`stocktwits_msgs`**: Float (count).\n",
        "3.  **`wiki_views`**: Float (count).\n",
        "4.  **`news_articles`**: Float (count).\n",
        "5.  **`google_trends`**: Float (index).\n",
        "\n",
        "*Note: `NaN` in attention columns represents \"No Coverage\", while `0.0` represents \"No Activity\".*\n",
        "\n",
        "## Usage\n",
        "\n",
        "The notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell, which demonstrates how to use the top-level `run_aimm_x_pipeline` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    config = load_study_configuration(\"config.yaml\")\n",
        "    \n",
        "    # 2. Load raw datasets (Example using synthetic generator provided in the notebook)\n",
        "    # In production, load from CSV/Parquet: pd.read_parquet(...)\n",
        "    df_raw_panel = generate_synthetic_panel(config)\n",
        "\n",
        "    # 3. Execute the entire replication study.\n",
        "    result = run_aimm_x_pipeline(df_raw_panel, config)\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(result.df_top_n.head())\n",
        "    print(result.audit_log)\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a `PipelineResult` object containing:\n",
        "-   **`config_snapshot`**: The resolved configuration dictionary used for the run.\n",
        "-   **`audit_log`**: A structured log of execution metadata, validation stats, and step completion.\n",
        "-   **`df_windows_raw`**: The complete set of detected windows with all scores and factors.\n",
        "-   **`df_windows_filtered`**: The subset of windows passing quality filters (warmup, artifacts).\n",
        "-   **`df_top_n`**: The top-ranked suspicious windows formatted for reporting.\n",
        "-   **`df_phi_summary`**: Summary statistics for factor contributions.\n",
        "-   **`intermediate_series`**: Dictionary containing computed time-series ($r$, $\\sigma$, $A$, $s$, z-scores) for debugging.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "explainable_market_integrity_monitoring_system/\n",
        "│\n",
        "├── explainable_market_integrity_monitoring_system_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                                  # Master configuration file\n",
        "├── requirements.txt                                             # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                                      # MIT Project License File\n",
        "└── README.md                                                    # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Universe:** `universe_tickers` list.\n",
        "-   **Detection Logic:** `baseline_window_B`, `theta_high`, `theta_low`, `gap_tolerance_g`.\n",
        "-   **Scoring Weights:** `alpha` weights for composite score, `omega` weights for integrity score.\n",
        "-   **Filters:** `exclude_warmup`, `max_z_score_cutoff`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **High-Frequency Data:** Adapting the pipeline for 5-minute or 1-minute bars.\n",
        "-   **Real-Time API Integration:** Connecting to live feeds for Reddit/Twitter data.\n",
        "-   **Advanced Normalization:** Implementing robust scalers (e.g., Median Absolute Deviation) for fat-tailed distributions.\n",
        "-   **Causal Inference:** Integrating Granger causality tests to determine lead-lag relationships between attention and price.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{neela2026aimmx,\n",
        "  title={AIMM-X: An Explainable Market Integrity Monitoring System Using Multi-Source Attention Signals and Transparent Scoring},\n",
        "  author={Neela, Sandeep},\n",
        "  journal={arXiv preprint arXiv:2601.15304v1},\n",
        "  year={2026}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2026). Explainable Market Integrity Monitoring System: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/explainable_market_integrity_monitoring_system\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Sandeep Neela** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, and PyYAML**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `explainable_market_integrity_monitoring_system_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "LAHOUlPn77KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*An Explainable Market Integrity Monitoring System with Multi-Source Attention Signals and Transparent Scoring*\"\n",
        "\n",
        "Authors: Sandeep Neela\n",
        "\n",
        "E-Journal Submission Date: 10 January 2026\n",
        "\n",
        "Link: https://arxiv.org/abs/2601.15304v1\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Market integrity monitoring is difficult because suspicious price/volume behavior can arise from many benign mechanisms, while modern detection systems often rely on opaque models that are hard to audit and communicate. We present AIMM-X, an explainable monitoring pipeline that combines market microstructure-style signals derived from OHLCV time series with multi-source public attention signals (e.g., news and online discussion proxies) to surface time windows that merit analyst review. The system detects candidate anomalous windows using transparent thresholding and aggregation, then assigns an interpretable integrity score decomposed into a small set of additive components, allowing practitioners to trace why a window was flagged and which factors drove the score. We provide an end-to-end, reproducible implementation that downloads data, constructs attention features, builds unified panels, detects windows, computes component signals, and generates summary figures/tables. Our goal is not to label manipulation, but to provide a practical, auditable screening tool that supports downstream investigation by compliance teams, exchanges, or researchers."
      ],
      "metadata": {
        "id": "5Lrne24r2fW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Executive Summary**\n",
        "\n",
        "The paper introduces **AIMM-X**, an open-source, explainable framework for detecting market manipulation and structural anomalies. Addressing the \"black-box\" nature and data exclusivity of proprietary surveillance systems (e.g., NASDAQ Smarts), the authors propose a methodology relying solely on **publicly accessible data** (OHLCV and social attention signals). The system identifies \"suspicious windows\"—intervals where price, volatility, and public attention exhibit statistically significant co-movement—and ranks them using a decomposable **Integrity Score ($M$)**. The framework is positioned not as an accusation engine, but as a **triage tool** for analysts, prioritizing transparency and reproducibility over opaque efficacy.\n",
        "\n",
        "### **Motivation and Problem Statement**\n",
        "\n",
        "The research identifies a critical gap in current market surveillance capabilities:\n",
        "*   **The Reproducibility Crisis:** Existing detection literature relies on proprietary order-book data (Level 3) and trader IDs, making independent validation impossible for academic researchers.\n",
        "*   **Black-Box Opacity:** Modern ML anomaly detection often lacks interpretability, failing to explain *why* a specific trading window was flagged—a requirement for regulatory enforcement.\n",
        "*   **The \"GameStop\" Paradigm:** Traditional volatility alerts fail to capture the coordinated, attention-driven dynamics characteristic of modern retail-driven market events (e.g., meme stock rallies).\n",
        "\n",
        "**Objective:** To build an auditable pipeline that surfaces anomalies using only public data, providing a \"white-box\" alternative to exchange-grade surveillance.\n",
        "\n",
        "### **Methodology and Architecture**\n",
        "\n",
        "The AIMM-X pipeline operates in four distinct stages:\n",
        "\n",
        "#### **A. Panel Construction & Feature Engineering**\n",
        "The system integrates financial time series with multi-source attention signals.\n",
        "*   **Market Data:** Daily OHLCV (Open, High, Low, Close, Volume) from Polygon.io.\n",
        "*   **Attention Signals ($A_{i,t}$):** A fused signal derived from five sources: Reddit, StockTwits, Wikipedia, News, and Google Trends. (Note: For this preprint, stylized proxies calibrated to literature were used).\n",
        "*   **Derived Metrics:** Log returns ($r_{i,t}$) and rolling volatility ($\\sigma_{i,t}$).\n",
        "\n",
        "#### **B. Statistical Deviation Detection**\n",
        "The system employs a rolling baseline approach to adapt to changing market regimes.\n",
        "*   **Baselines:** Rolling mean ($\\mu$) and standard deviation ($\\hat{\\sigma}$) calculated over a window $B=20$ days.\n",
        "*   **Z-Scores:** Standardized deviations are computed for returns ($z^{(r)}$), volatility ($z^{(\\sigma)}$), and attention ($z^{(A)}$).\n",
        "*   **Composite Strength Score ($s_{i,t}$):** A weighted sum of the absolute Z-scores across channels.\n",
        "\n",
        "#### **C. Hysteresis-Based Segmentation**\n",
        "To avoid fragmented alerts, the system uses **hysteresis thresholding** (similar to Canny edge detection in computer vision):\n",
        "*   **$\\theta_{high}$ (3.0):** The threshold required to *initiate* a suspicious window.\n",
        "*   **$\\theta_{low}$ (2.0):** The lower threshold allowing a window to *continue*.\n",
        "*   **Constraints:** Windows must meet minimum length requirements and bridge small gaps (gap tolerance).\n",
        "\n",
        "#### **D. The Interpretable Integrity Score ($M$)**\n",
        "This is the core econometric contribution. Detected windows are ranked by a score $M(w)$, which is a linear combination of six interpretable factors ($\\phi$):\n",
        "$$M(w) = \\sum_{k=1}^{6} \\omega_k \\cdot \\phi_k(w)$$\n",
        "\n",
        "*   **$\\phi_1$ (Return Shock):** Magnitude of abnormal returns (squared Z-scores).\n",
        "*   **$\\phi_2$ (Volatility Anomaly):** Unusual volatility independent of direction.\n",
        "*   **$\\phi_3$ (Attention Spike):** Magnitude of social/news attention surge.\n",
        "*   **$\\phi_4$ (Co-movement):** Correlation between price/volatility and attention (detecting coordination).\n",
        "*   **$\\phi_5$ (Recurrence):** Frequency of similar windows in short succession.\n",
        "*   **$\\phi_6$ (Disagreement):** Penalty for divergence between attention sources (e.g., Reddit spikes while News is silent).\n",
        "\n",
        "\n",
        "### **Experimental Design**\n",
        "\n",
        "*   **Universe:** 24 tickers selected to stress-test the system, including Meme stocks (GME, AMC), Large-cap Tech (META, NVDA), Crypto-exposed (MSTR, COIN), and ETFs (SPY, TLT).\n",
        "*   **Period:** January 8, 2024 – December 31, 2024.\n",
        "*   **Data Resolution:** Daily bars (a limitation acknowledged for future high-frequency integration).\n",
        "\n",
        "\n",
        "### **Key Results and Findings**\n",
        "\n",
        "The system detected **233 suspicious windows** across the 24 tickers.\n",
        "\n",
        "*   **Factor Dominance:** In the current configuration, $\\phi_1$ (Return Shock) dominated the scoring variance due to a lack of normalization, leading to a heavy-tailed score distribution.\n",
        "*   **Top Detection (META, Jan 10-12, 2024):** The highest-scoring window ($M \\approx 7.1M$).\n",
        "    *   *Analysis:* Coincided with market-wide inflation data volatility.\n",
        "    *   *Verdict:* False positive for manipulation, but true positive for \"market stress.\"\n",
        "*   **Meme Stock Dynamics (GME, May 2-17, 2024):**\n",
        "    *   Detected a 12-bar window characterized by sustained price-attention co-movement.\n",
        "    *   Demonstrated the system's ability to capture \"pump\" dynamics distinct from sudden news shocks.\n",
        "*   **Crypto Correlation (MSTR):**\n",
        "    *   Windows aligned with Bitcoin volatility, demonstrating the system's sensitivity to sector-specific drivers.\n",
        "\n",
        "\n",
        "### **Critical Analysis & Limitations**\n",
        "\n",
        "As a reviewer of the methodology, several points regarding rigor and validity stand out:\n",
        "\n",
        "*   **The Ground Truth Problem:** The paper rightly acknowledges that \"ground truth\" in manipulation is unobservable without regulatory enforcement data. The authors propose a \"triangulation\" validation strategy (consistency, face validity, and retrospective enforcement checks).\n",
        "*   **Data Granularity:** The use of daily bars is a significant constraint. Intraday manipulation (e.g., momentum ignition) requires minute-level or tick-level data, which is planned for Phase 2.\n",
        "*   **Proxy Data:** The use of \"stylized attention proxies\" rather than live API feeds in this preprint limits the immediate empirical applicability, though it validates the *pipeline logic*.\n",
        "*   **Triage vs. Verdict:** The authors ethically frame AIMM-X as a triage system. It produces statistical evidence for human review, not definitive proof of wrongdoing, mitigating risks of false accusation.\n",
        "\n",
        "\n",
        "### **Conclusion and Future Roadmap**\n",
        "\n",
        "AIMM-X represents a step toward **democratizing market surveillance**. By decoupling detection from proprietary data, it enables academic scrutiny of surveillance logic.\n",
        "\n",
        "**Future Work Proposed:**\n",
        "1.  Integration of 5-minute OHLCV bars to localize anomalies.\n",
        "2.  Implementation of authenticated APIs for real-time attention data.\n",
        "3.  **Factor Normalization:** Standardizing $\\phi$ factors to prevent return shocks from masking attention anomalies.\n",
        "4.  **Causal Inference:** Moving beyond correlation to Granger causality tests between attention and price.\n",
        "\n",
        "**Final Assessment:** The paper provides a robust, mathematically sound framework for *explainable* financial anomaly detection. While currently limited by data resolution, the architecture's transparency offers a significant contribution to the field of computational finance and regulatory technology (RegTech)."
      ],
      "metadata": {
        "id": "mjzgWlEW_KUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "QXHNsct7iML6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  AIMM-X: An Explainable Market Integrity Monitoring System\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"An Explainable Market Integrity Monitoring\n",
        "#  System with Multi-Source Attention Signals and Transparent Scoring\" by\n",
        "#  Sandeep Neela (2026). It delivers a computationally tractable system for\n",
        "#  surveillance triage, enabling the detection of suspicious trading windows\n",
        "#  characterized by the joint deviation of price, volatility, and public attention.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Multi-source attention signal fusion via weighted aggregation\n",
        "#  • Robust rolling baseline estimation for regime-adaptive anomaly detection\n",
        "#  • Hysteresis-based segmentation (Schmitt Trigger) for window identification\n",
        "#  • Interpretable Integrity Score (M) decomposition into additive phi-factors\n",
        "#  • Rank-based filtering to surface high-priority alerts for analyst review\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Vectorized pandas/numpy operations for efficient panel data processing\n",
        "#  • Strict handling of look-ahead bias via lagged baseline construction\n",
        "#  • Precise missingness semantics (NaN vs Zero) for sparse attention data\n",
        "#  • Comprehensive audit logging and artifact generation for reproducibility\n",
        "#  • Modular architecture supporting independent validation of each pipeline stage\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Neela, S. (2026). An Explainable Market Integrity Monitoring System with\n",
        "#  Multi-Source Attention Signals and Transparent Scoring.\n",
        "#  arXiv preprint arXiv:2601.15304v1.\n",
        "#  https://arxiv.org/abs/2601.15304v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from typing import (\n",
        "    Any,\n",
        "    Dict,\n",
        "    List,\n",
        "    NamedTuple,\n",
        "    Optional,\n",
        "    Tuple,\n",
        "    Union\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "\n",
        "# Optional dependency for precise exchange calendar alignment\n",
        "try:\n",
        "    import exchange_calendars as xcals\n",
        "except ImportError:\n",
        "    xcals = None\n",
        "\n",
        "# Configure logging for the pipeline\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "dDw2GtexiSH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "-KzgGwgliUCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "## **Discussion of Inputs, Processes, Outputs, and Research Role of Key Callables**\n",
        "\n",
        "Here is the granular analysis of the Inputs, Processes, Outputs, and Research Role for each of the 17 final callables in the AIMM-X pipeline:\n",
        "\n",
        "### 1. `validate_study_config` (Task 1 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `config` (Dictionary): The raw JSON-like configuration object containing metadata, schemas, algorithm parameters, and reproducibility settings.\n",
        "*   **Processes:**\n",
        "    *   **Structural Verification:** Checks for the presence of all 11 required top-level keys (e.g., `meta`, `deviation_detection`, `scoring_model`).\n",
        "    *   **Constraint Validation:** Enforces methodological constraints such as universe size ($N=24$), hysteresis thresholds ($\\theta_{\\text{high}} > \\theta_{\\text{low}}$), and weight positivity ($\\alpha \\ge 0, \\omega \\ge 0$).\n",
        "    *   **Determinism Check:** Verifies the existence of reproducibility artifacts like explicit session lists and random seeds.\n",
        "*   **Outputs:**\n",
        "    *   `validated_config` (Dictionary): The validated configuration object, guaranteed to be structurally sound and methodologically consistent.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **Configuration Management** requirement (Section 3.5), ensuring that all parameters governing the pipeline are \"specified in `config.json`, enabling reproducible runs and systematic parameter sweeps.\" It enforces the logical consistency of the detection thresholds defined in Section 4.3.\n",
        "\n",
        "### 2. `validate_panel_schema` (Task 2 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_raw_panel` (DataFrame): The ingested raw data containing market microstructure and attention signals.\n",
        "    *   `config` (Dictionary): The validated study configuration.\n",
        "*   **Processes:**\n",
        "    *   **Index Inspection:** Verifies the DataFrame possesses a MultiIndex with levels `['date', 'ticker']` that is unique and monotonic.\n",
        "    *   **Column Audit:** Confirms the presence and correct data types of all required columns (e.g., `close_price`, `reddit_posts`), specifically ensuring attention columns support `NaN` for missing coverage.\n",
        "    *   **Universe Verification:** Checks that the set of tickers in the data exactly matches the 24-ticker universe defined in the configuration.\n",
        "*   **Outputs:**\n",
        "    *   `is_valid` (Boolean): A flag indicating whether the schema is compliant. Raises an exception if false.\n",
        "*   **Research Role:**\n",
        "    *   Enforces the **Panel Construction** prerequisites (Section 4.1), ensuring the data structure supports the \"unified time-series panel\" required for vectorized operations. It validates the **Ticker Universe Selection** (Section 3.1) to ensure the study operates on the specific set of 24 high-attention securities.\n",
        "\n",
        "### 3. `cleanse_panel` (Task 3 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_raw_panel` (DataFrame): The raw input panel.\n",
        "    *   `config` (Dictionary): Configuration defining column names and types.\n",
        "*   **Processes:**\n",
        "    *   **OHLC Consistency Check:** Identifies rows where $H_{i,t} < \\max(O_{i,t}, C_{i,t})$ or $L_{i,t} > \\min(O_{i,t}, C_{i,t})$.\n",
        "    *   **Liquidity Validation:** Flags rows with non-positive close prices ($C_{i,t} \\le 0$) or zero volume ($V_{i,t} \\le 0$).\n",
        "    *   **Quarantine:** Segregates invalid rows into a separate DataFrame.\n",
        "    *   **Type Enforcement:** Casts attention columns to `float64` to strictly distinguish between zero activity ($0.0$) and missing coverage (`NaN`).\n",
        "*   **Outputs:**\n",
        "    *   `df_clean` (DataFrame): The subset of methodologically valid rows.\n",
        "    *   `df_quarantine` (DataFrame): The subset of invalid rows for audit.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **Data Quality and Preprocessing** steps (Section 3.2), specifically the checks for \"price discontinuities,\" \"plausible volume figures,\" and \"logical constraints\" on OHLC relationships. It ensures the \"Missingness handling\" logic (Section 3.3) is supported by the data types.\n",
        "\n",
        "### 4. `enforce_trading_calendar` (Task 4 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_clean` (DataFrame): The cleansed panel data.\n",
        "    *   `config` (Dictionary): Configuration containing the canonical session list or generation rules.\n",
        "*   **Processes:**\n",
        "    *   **Calendar Generation:** Constructs the authoritative list of NYSE/Nasdaq trading sessions for the study period (excluding weekends and holidays).\n",
        "    *   **Alignment Check:** Iterates through every ticker to verify exact one-to-one mapping between data rows and canonical sessions.\n",
        "    *   **Audit Reporting:** Identifies and logs any missing sessions (gaps) or extra sessions (calendar mismatches).\n",
        "*   **Outputs:**\n",
        "    *   `report` (Dictionary): A summary of alignment status and specific mismatches per ticker.\n",
        "*   **Research Role:**\n",
        "    *   Ensures the **Temporal Coverage** fidelity (Section 3.2), guaranteeing the analysis covers the exact \"248 trading days per ticker\" specified in the experimental design. This prevents artifacts from non-trading days affecting rolling baseline calculations.\n",
        "\n",
        "### 5. `align_attention_sources` (Task 5 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_clean` (DataFrame): The cleansed panel data.\n",
        "    *   `config` (Dictionary): Configuration defining source mappings and alignment rules.\n",
        "*   **Processes:**\n",
        "    *   **Extraction:** Isolates raw attention columns mapped to canonical names (e.g., `reddit`, `wiki`).\n",
        "    *   **Rule Application:** Applies source-specific filling logic. Crucially, it applies forward-filling (`ffill`) *only* to sources where it is methodologically appropriate (e.g., step-function data like Wikipedia views), while preserving raw sparsity for event-driven sources (e.g., Reddit posts).\n",
        "    *   **Semantic Validation:** Confirms that `NaN` (no coverage) and `0.0` (zero activity) remain distinct.\n",
        "*   **Outputs:**\n",
        "    *   `aligned_series` (Dictionary of Series): The aligned attention time-series ready for normalization.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **Resampling and Alignment** logic (Section 3.3), ensuring that \"all sources align temporally with OHLCV data\" while respecting the specific \"natural cadences\" of different attention proxies.\n",
        "\n",
        "### 6. `normalize_attention_sources` (Task 6 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `aligned_series` (Dictionary of Series): The aligned raw attention signals.\n",
        "    *   `config` (Dictionary): Configuration defining the baseline window $B$ and stability constant $\\epsilon$.\n",
        "*   **Processes:**\n",
        "    *   **Rolling Normalization:** For each source $s$, computes a rolling z-score using a strictly lagged baseline to prevent look-ahead bias.\n",
        "    *   **Transformation:** Applies the formula $\\tilde{a}_{s,i,t} = (a_{s,i,t} - \\mu_{t}) / (\\sigma_{t} + \\epsilon)$.\n",
        "*   **Outputs:**\n",
        "    *   `normalized_dict` (Dictionary of Series): The normalized signals $\\tilde{a}_{s,i,t}$.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **Attention Signal Construction** requirement (Section 3.3) to produce \"resampled and normalized source signals\" $\\tilde{a}_{s,t}$ suitable for the weighted sum fusion in Equation (1).\n",
        "\n",
        "### 7. `fuse_attention` (Task 7 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `normalized_dict` (Dictionary of Series): The normalized attention signals.\n",
        "    *   `config` (Dictionary): Configuration containing fusion weights $w_s$.\n",
        "*   **Processes:**\n",
        "    *   **Weighted Aggregation:** Computes the linear combination of normalized sources.\n",
        "    *   **Strict Propagation:** Enforces logic where if *any* source is `NaN` (missing coverage), the fused result is `NaN`.\n",
        "*   **Outputs:**\n",
        "    *   `A_series` (Series): The fused attention signal $A_{i,t}$.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (1)** from Section 3.3:\n",
        "        $$ A_t = \\sum_{s \\in \\mathcal{S}} w_s \\cdot \\tilde{a}_{s,t} $$\n",
        "        This creates the unified \"Attention\" channel used in the multi-source detection logic.\n",
        "\n",
        "### 8. `compute_log_returns` (Task 8 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_clean` (DataFrame): The cleansed panel data containing `close_price`.\n",
        "    *   `config` (Dictionary): Configuration (passed for consistency).\n",
        "*   **Processes:**\n",
        "    *   **Log Transformation:** Computes the natural logarithm of the ratio of consecutive close prices.\n",
        "    *   **Grouping:** Ensures calculations are isolated per ticker.\n",
        "*   **Outputs:**\n",
        "    *   `r_series` (Series): The log return series $r_{i,t}$.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (2)** from Section 4.1:\n",
        "        $$ r_{i,t} = \\log \\left( \\frac{C_{i,t}}{C_{i,t-1}} \\right) $$\n",
        "        This transforms non-stationary prices into stationary returns for statistical analysis.\n",
        "\n",
        "### 9. `compute_volatility` (Task 9 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `r_series` (Series): The log return series.\n",
        "    *   `config` (Dictionary): Configuration defining lookback $L$ and $\\epsilon$.\n",
        "*   **Processes:**\n",
        "    *   **Lagging:** Shifts returns by 1 to ensure the volatility estimate for time $t$ uses only past data ($t-1 \\dots t-L$).\n",
        "    *   **RMS Calculation:** Computes the rolling root-mean-square of the lagged returns.\n",
        "*   **Outputs:**\n",
        "    *   `sigma_series` (Series): The realized volatility proxy $\\sigma_{i,t}$.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (3)** from Section 4.1:\n",
        "        $$ \\sigma_{i,t} = \\sqrt{\\frac{1}{L} \\sum_{j=1}^{L} r_{i,t-j}^2 + \\epsilon} $$\n",
        "        This provides the \"Volatility proxy\" used as a distinct channel in the detection model.\n",
        "\n",
        "### 10. `compute_baselines_and_zscores` (Task 10 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `r_series`, `sigma_series`, `A_series` (Series): The three data channels.\n",
        "    *   `config` (Dictionary): Configuration defining baseline window $B$.\n",
        "*   **Processes:**\n",
        "    *   **Baseline Estimation:** Computes rolling mean $\\mu^{(x)}$ and standard deviation $\\hat{\\sigma}^{(x)}$ over a lagged window of length $B$.\n",
        "    *   **Standardization:** Computes the z-score for each observation relative to its baseline.\n",
        "*   **Outputs:**\n",
        "    *   `z_r`, `z_sigma`, `z_A` (Series): The standardized deviation series.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equations (4), (5), and (6)** from Section 4.2:\n",
        "        $$ z_{i,t}^{(x)} = \\frac{x_{i,t} - \\mu_{i,t}^{(x)}}{\\hat{\\sigma}_{i,t}^{(x)} + \\epsilon} $$\n",
        "        This is the core \"Statistical Deviation Metric\" that normalizes diverse signals into a common unitless scale for aggregation.\n",
        "\n",
        "### 11. `compute_composite_strength` (Task 11 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `z_r`, `z_sigma`, `z_A` (Series): The z-score series.\n",
        "    *   `config` (Dictionary): Configuration defining weights $\\alpha_r, \\alpha_\\sigma, \\alpha_A$.\n",
        "*   **Processes:**\n",
        "    *   **Magnitude Extraction:** Takes the absolute value of return z-scores (if configured for two-sided detection).\n",
        "    *   **Weighted Summation:** Aggregates the z-scores into a single scalar strength value.\n",
        "*   **Outputs:**\n",
        "    *   `s_series` (Series): The composite strength score $s_{i,t}$.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (7)** from Section 4.3:\n",
        "        $$ s_{i,t} = \\alpha_r|z_{i,t}^{(r)}| + \\alpha_\\sigma z_{i,t}^{(\\sigma)} + \\alpha_A z_{i,t}^{(A)} $$\n",
        "        This \"Multi-channel fusion\" step creates the primary signal used for window segmentation.\n",
        "\n",
        "### 12. `segment_windows_hysteresis` (Task 12 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `s_series` (Series): The composite strength score.\n",
        "    *   `config` (Dictionary): Configuration defining thresholds $\\theta_{\\text{high}}, \\theta_{\\text{low}}$ and gap tolerance $g$.\n",
        "*   **Processes:**\n",
        "    *   **Schmitt Trigger Logic:** Iterates through the time series, triggering a window when $s > \\theta_{\\text{high}}$ and sustaining it while $s > \\theta_{\\text{low}}$, allowing for short gaps of length $\\le g$.\n",
        "    *   **Aggregation:** Collects all detected windows across all tickers.\n",
        "*   **Outputs:**\n",
        "    *   `df_windows_raw` (DataFrame): A list of detected suspicious windows defined by start and end timestamps.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **Hysteresis-based segmentation** algorithm described in Section 4.3. This converts the continuous strength signal into discrete \"Suspicious Windows\" for analysis.\n",
        "\n",
        "### 13. `compute_phi_factors` (Task 13 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_windows_raw` (DataFrame): The detected windows.\n",
        "    *   `z_r`, `z_sigma`, `z_A` (Series): The underlying z-scores.\n",
        "    *   `config` (Dictionary): Configuration.\n",
        "*   **Processes:**\n",
        "    *   **Slicing:** Extracts the z-score data corresponding to the time interval of each window.\n",
        "    *   **Factor Computation:** Calculates the six interpretable factors ($\\phi_1 \\dots \\phi_6$) based on the sliced data (e.g., sum of squares, correlation).\n",
        "*   **Outputs:**\n",
        "    *   `df_phi` (DataFrame): The windows DataFrame enriched with factor values.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements the **Phi-Signal Definitions** (Equations 8–13) from Section 4.4.1. This step generates the \"evidence\" (Return shock, Volatility anomaly, Attention spike, Alignment) that explains *why* a window was flagged.\n",
        "\n",
        "### 14. `compute_integrity_scores` (Task 14 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_phi` (DataFrame): Windows with computed factors.\n",
        "    *   `config` (Dictionary): Configuration defining weights $\\omega_k$.\n",
        "*   **Processes:**\n",
        "    *   **Linear Combination:** Computes the weighted sum of phi factors.\n",
        "    *   **Attribution:** Stores the individual contribution of each factor to the total score.\n",
        "*   **Outputs:**\n",
        "    *   `df_scored` (DataFrame): Windows with the final Integrity Score $M$ and decomposition columns.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (14)** and the **Factor Decomposition** logic (Equation 16) from Section 4.4.2:\n",
        "        $$ M(w) = \\sum_{k=1}^{6} \\omega_k \\cdot \\phi_k(w) $$\n",
        "        This produces the \"Interpretable Integrity Score\" used to rank anomalies.\n",
        "\n",
        "### 15. `rank_and_filter_windows` (Task 15 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_scored` (DataFrame): Scored windows.\n",
        "    *   `z_r`, `z_sigma`, `z_A` (Series): Z-scores for artifact checking.\n",
        "    *   `config` (Dictionary): Configuration defining filters.\n",
        "*   **Processes:**\n",
        "    *   **Ranking:** Computes the percentile rank of each window's score $M$.\n",
        "    *   **Warmup Filtering:** Excludes windows occurring before baselines are fully established.\n",
        "    *   **Artifact Filtering:** Excludes windows containing extreme z-score outliers (e.g., $> 20\\sigma$).\n",
        "*   **Outputs:**\n",
        "    *   `df_ranked` (DataFrame): The full set of ranked windows.\n",
        "    *   `df_filtered` (DataFrame): The subset of windows passing quality filters.\n",
        "*   **Research Role:**\n",
        "    *   Accurately implements **Equation (15)** for **Rank percentiles** (Section 4.4.2) and applies the \"warm-up periods\" and \"artifact filtering\" described in Section 5.3 to produce a clean list for reporting.\n",
        "\n",
        "### 16. `export_artifacts` (Task 16 Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_ranked`, `df_filtered` (DataFrame): The processed window sets.\n",
        "    *   `config` (Dictionary): Configuration.\n",
        "*   **Processes:**\n",
        "    *   **Formatting:** Selects and orders columns for the final output tables.\n",
        "    *   **Top-N Selection:** Extracts the highest-scoring windows for the summary report.\n",
        "    *   **Statistics:** Computes summary statistics (mean, max, nonzero%) for the phi factors.\n",
        "*   **Outputs:**\n",
        "    *   `artifacts` (Dictionary): A collection of DataFrames representing the study's final deliverables.\n",
        "*   **Research Role:**\n",
        "    *   Generates the **Empirical Contributions** (Section 1.4), specifically the \"window lists, rankings, factor attributions, and case study reports\" (Table 4, Table 5) required to communicate findings to analysts.\n",
        "\n",
        "### 17. `run_aimm_x_pipeline` (Task 17 Top-Level Orchestrator)\n",
        "\n",
        "*   **Inputs:**\n",
        "    *   `df_raw_panel` (DataFrame): The raw input data.\n",
        "    *   `config` (Dictionary): The study configuration.\n",
        "*   **Processes:**\n",
        "    *   **Sequential Execution:** Calls callables 1 through 16 in the strict dependency order defined by the pipeline architecture.\n",
        "    *   **State Management:** Passes intermediate results (e.g., `df_clean`, `z_scores`) between stages.\n",
        "    *   **Audit Logging:** Records the success/failure and metadata of each step.\n",
        "*   **Outputs:**\n",
        "    *   `PipelineResult` (NamedTuple): A comprehensive container holding the config snapshot, audit log, all data artifacts, and intermediate series.\n",
        "*   **Research Role:**\n",
        "    *   Implements the **End-to-End Pipeline** (Algorithm 1 in Section 4.5), providing the \"reproducible end-to-end pipeline\" that integrates data ingestion, detection, scoring, and reporting into a single executable unit.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "Below is a Python code snippet which uses synthetically generated data to illustrate accurate use of the AIMM-X Pipeline callables:\n",
        "\n",
        "```python\n",
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "# AIMM-X Pipeline Usage Example\n",
        "# Context: \"The Democratization of Market Integrity Verification\"\n",
        "# ==============================================================================\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "# Import all essential Python modules\n",
        "\n",
        "# Configure logging to stdout for demonstration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(\"AIMM-X_Demo\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 1: Configuration Management\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1 Implementation: Read config.yaml into memory\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_study_configuration(filepath: str = \"config.yaml\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Reads the YAML configuration file from disk and returns it as a Python dictionary.\n",
        "\n",
        "    This function serves as the entry point for the configuration management system,\n",
        "    loading the hierarchical parameter set that governs the entire AIMM-X pipeline.\n",
        "    It ensures that the configuration is accessible as a standard Python dictionary\n",
        "    for downstream validation and usage.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str, optional\n",
        "        The path to the YAML configuration file. Defaults to \"config.yaml\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The parsed configuration dictionary containing metadata, schemas, and parameters.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified configuration file does not exist at the given path.\n",
        "    yaml.YAMLError\n",
        "        If the file contains invalid YAML syntax that cannot be parsed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the configuration file in read mode using a context manager for safety\n",
        "        with open(filepath, 'r') as f:\n",
        "            # Parse the YAML content into a Python dictionary using safe_load to prevent code execution\n",
        "            config = yaml.safe_load(f)\n",
        "        \n",
        "        # Log successful loading of the configuration for audit purposes\n",
        "        logger.info(f\"Successfully loaded configuration from {filepath}\")\n",
        "        \n",
        "        # Return the parsed configuration dictionary\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Log an error if the file is missing, which is a critical failure for the pipeline\n",
        "        logger.error(f\"Configuration file not found at {filepath}\")\n",
        "        # Re-raise the exception to halt execution\n",
        "        raise\n",
        "\n",
        "    except yaml.YAMLError as e:\n",
        "        # Log an error if the YAML syntax is invalid\n",
        "        logger.error(f\"Error parsing YAML file: {e}\")\n",
        "        # Re-raise the exception to halt execution\n",
        "        raise\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2: Synthetic Data Generation (df_raw_panel)\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_synthetic_panel(config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a professional-grade synthetic DataFrame matching the AIMM-X schema.\n",
        "\n",
        "    This function creates a realistic synthetic dataset for testing and demonstration\n",
        "    purposes. It strictly enforces the methodological constraints defined in the\n",
        "    AIMM-X paper, including exchange trading calendar alignment, OHLC consistency,\n",
        "    and specific missingness semantics for attention signals.\n",
        "\n",
        "    Processes:\n",
        "    1.  **Calendar Generation:** Constructs a date index approximating the NYSE calendar,\n",
        "        excluding weekends and US Federal holidays.\n",
        "    2.  **Microstructure Simulation:** Generates price paths using Geometric Brownian Motion (GBM)\n",
        "        and derives Open, High, Low, Close (OHLC) values that satisfy logical constraints\n",
        "        (e.g., High >= max(Open, Close)). Generates strictly positive Volume.\n",
        "    3.  **Attention Signal Simulation:** Generates synthetic attention data using Poisson\n",
        "        processes for sparse sources (Reddit, News) and log-normal distributions for\n",
        "        continuous sources (Wikipedia).\n",
        "    4.  **Missingness Injection:** Randomly injects NaNs into attention columns to simulate\n",
        "        \"No Coverage\" (API downtime), distinct from \"No Activity\" (0).\n",
        "    5.  **Assembly:** Concatenates per-ticker data into a single MultiIndex DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary containing universe definition, date range,\n",
        "        and reproducibility seeds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The synthetic raw panel DataFrame (`df_raw_panel`) with MultiIndex ['date', 'ticker'].\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    KeyError\n",
        "        If required configuration keys (e.g., 'meta', 'universe_tickers') are missing.\n",
        "    \"\"\"\n",
        "    # Log the start of the synthetic data generation process\n",
        "    logger.info(\"Generating synthetic panel data...\")\n",
        "    \n",
        "    # 1. Calendar Generation (Approximating NYSE)\n",
        "    # Extract start and end dates from the configuration\n",
        "    start_date = config[\"meta\"][\"date_range\"][\"start\"]\n",
        "    end_date = config[\"meta\"][\"date_range\"][\"end\"]\n",
        "    \n",
        "    # Create a CustomBusinessDay offset using the US Federal Holiday Calendar\n",
        "    # This ensures the generated dates align with a realistic trading calendar (Constraint A)\n",
        "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
        "    \n",
        "    # Generate the range of dates using the custom offset\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq=us_bd)\n",
        "    \n",
        "    # Calculate the number of dates for array sizing\n",
        "    n_dates = len(dates)\n",
        "    \n",
        "    # Extract the list of tickers from the configuration\n",
        "    tickers = config[\"meta\"][\"universe_tickers\"]\n",
        "    \n",
        "    # Initialize a list to hold per-ticker DataFrames\n",
        "    frames = []\n",
        "    \n",
        "    # Set the random seed for reproducibility as specified in the config\n",
        "    np.random.seed(config[\"preprint_attention_proxy_reproducibility\"][\"random_seed\"])\n",
        "\n",
        "    # Iterate through each ticker to generate its specific data\n",
        "    for ticker in tickers:\n",
        "        # --- Market Microstructure Generation ---\n",
        "        # Parameters for Geometric Brownian Motion (GBM)\n",
        "        dt = 1/252  # Time step (1 trading day)\n",
        "        mu = 0.10   # Annualized drift\n",
        "        sigma = 0.30 # Annualized volatility\n",
        "        \n",
        "        # Generate an initial price uniformly between $10 and $200\n",
        "        s0 = np.random.uniform(10, 200)\n",
        "        \n",
        "        # Generate daily log returns using a normal distribution\n",
        "        # Equation: r_t ~ N((mu - 0.5 * sigma^2) * dt, sigma * sqrt(dt))\n",
        "        returns = np.random.normal((mu - 0.5 * sigma**2) * dt, sigma * np.sqrt(dt), n_dates)\n",
        "        \n",
        "        # Calculate the price path using the cumulative sum of log returns\n",
        "        # Equation: P_t = P_0 * exp(sum(r_1...r_t))\n",
        "        price_path = s0 * np.exp(np.cumsum(returns))\n",
        "        \n",
        "        # Derive OHLC values to strictly satisfy constraints\n",
        "        # Open price is modeled as close to the previous day's close (or current price path)\n",
        "        open_prices = price_path * np.random.uniform(0.99, 1.01, n_dates)\n",
        "        \n",
        "        # Close price is set to the generated price path\n",
        "        close_prices = price_path\n",
        "        \n",
        "        # High price must be >= max(Open, Close) to satisfy QC constraints\n",
        "        # Calculate the maximum of Open and Close for each day\n",
        "        max_oc = np.maximum(open_prices, close_prices)\n",
        "        # Generate High prices slightly above this maximum\n",
        "        high_prices = max_oc * np.random.uniform(1.001, 1.02, n_dates)\n",
        "        \n",
        "        # Low price must be <= min(Open, Close) to satisfy QC constraints\n",
        "        # Calculate the minimum of Open and Close for each day\n",
        "        min_oc = np.minimum(open_prices, close_prices)\n",
        "        # Generate Low prices slightly below this minimum\n",
        "        low_prices = min_oc * np.random.uniform(0.98, 0.999, n_dates)\n",
        "        \n",
        "        # Generate Volume using a log-normal distribution to simulate realistic trading volume\n",
        "        volume = np.random.lognormal(mean=14, sigma=0.5, size=n_dates).astype(int)\n",
        "        # Enforce strictly positive volume (Constraint B)\n",
        "        volume = np.maximum(volume, 100)\n",
        "        \n",
        "        # Calculate Volume Weighted Average Price (VWAP) approximation\n",
        "        # Note: This is a simplified approximation (Typical Price) for synthetic purposes\n",
        "        vwap = (high_prices + low_prices + close_prices) / 3\n",
        "        \n",
        "        # Generate Number of Transactions as a fraction of volume\n",
        "        txs = (volume / np.random.uniform(50, 200, n_dates)).astype(int)\n",
        "\n",
        "        # --- Attention Signal Generation ---\n",
        "        # Generate sparse, Poisson-like data for Reddit, StockTwits, and News\n",
        "        # 0 means \"no activity\" (valid observation of zero events)\n",
        "        reddit = np.random.poisson(lam=5, size=n_dates).astype(float)\n",
        "        stocktwits = np.random.poisson(lam=20, size=n_dates).astype(float)\n",
        "        news = np.random.poisson(lam=1, size=n_dates).astype(float)\n",
        "        \n",
        "        # Generate step-like, continuous data for Wikipedia and Google Trends\n",
        "        wiki = np.random.lognormal(mean=8, sigma=1, size=n_dates)\n",
        "        trends = np.random.uniform(0, 100, size=n_dates)\n",
        "        \n",
        "        # Inject Missingness (NaN) to simulate API downtime (Constraint C)\n",
        "        # This distinguishes \"No Coverage\" (NaN) from \"No Activity\" (0)\n",
        "        # Mask 5% of data as NaN for each attention source\n",
        "        for arr in [reddit, stocktwits, news, wiki, trends]:\n",
        "            # Create a boolean mask where True indicates a missing value (5% probability)\n",
        "            mask = np.random.choice([True, False], size=n_dates, p=[0.05, 0.95])\n",
        "            # Apply the mask to set values to NaN\n",
        "            arr[mask] = np.nan\n",
        "\n",
        "        # --- Assembly ---\n",
        "        # Create a DataFrame for the current ticker with all generated columns\n",
        "        df_ticker = pd.DataFrame({\n",
        "            \"open_price\": open_prices,\n",
        "            \"high_price\": high_prices,\n",
        "            \"low_price\": low_prices,\n",
        "            \"close_price\": close_prices,\n",
        "            \"volume\": volume,\n",
        "            \"volume_weighted_average_price\": vwap,\n",
        "            \"number_of_transactions\": txs,\n",
        "            \"reddit_posts\": reddit,\n",
        "            \"stocktwits_msgs\": stocktwits,\n",
        "            \"wiki_views\": wiki,\n",
        "            \"news_articles\": news,\n",
        "            \"google_trends\": trends\n",
        "        }, index=dates)\n",
        "        \n",
        "        # Add the ticker column to the DataFrame\n",
        "        df_ticker[\"ticker\"] = ticker\n",
        "        \n",
        "        # Reset the index to make 'date' a column, rename it, and then set the MultiIndex\n",
        "        df_ticker = df_ticker.reset_index().rename(columns={\"index\": \"date\"})\n",
        "        df_ticker = df_ticker.set_index([\"date\", \"ticker\"])\n",
        "        \n",
        "        # Append the ticker's DataFrame to the list\n",
        "        frames.append(df_ticker)\n",
        "\n",
        "    # Concatenate all ticker DataFrames into a single panel\n",
        "    df_raw_panel = pd.concat(frames)\n",
        "    \n",
        "    # Sort the MultiIndex by ticker then date to ensure monotonicity (Constraint D)\n",
        "    df_raw_panel = df_raw_panel.sort_index(level=[\"ticker\", \"date\"])\n",
        "    \n",
        "    # Log the shape of the generated panel for verification\n",
        "    logger.info(f\"Synthetic panel generated. Shape: {df_raw_panel.shape}\")\n",
        "    \n",
        "    # Return the final synthetic DataFrame\n",
        "    return df_raw_panel\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3: Pipeline Execution\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Setup Environment\n",
        "    create_dummy_config_file()\n",
        "    \n",
        "    # 2. Load Configuration\n",
        "    config = load_study_configuration(\"config.yaml\")\n",
        "    \n",
        "    # 3. Generate Data\n",
        "    df_raw_panel = generate_synthetic_panel(config)\n",
        "    \n",
        "    # 4. Execute Pipeline\n",
        "    # Note: Assuming that the required Python modules have been imported\n",
        "    # Note: Assuming that all the orchestrator callables (and their utilities) are available in the namespace\n",
        "    # Note: Assuming 'run_aimm_x_pipeline' is available in the namespace\n",
        "    # (In a real script, this would be imported from the module)\n",
        "    try:\n",
        "        logger.info(\"Initiating AIMM-X Pipeline...\")\n",
        "        result = run_aimm_x_pipeline(df_raw_panel, config)\n",
        "        \n",
        "        # 5. Inspect Results\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"AIMM-X PIPELINE EXECUTION REPORT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(f\"\\n[Audit Log Summary]\")\n",
        "        print(f\"Steps Completed: {result.audit_log['steps_completed']}\")\n",
        "        print(f\"Raw Windows Detected: {result.audit_log.get('raw_windows_count', 0)}\")\n",
        "        print(f\"Filtered Windows: {result.audit_log.get('filtered_windows_count', 0)}\")\n",
        "        \n",
        "        print(f\"\\n[Top 5 Suspicious Windows (Triage List)]\")\n",
        "        if not result.df_top_n.empty:\n",
        "            print(result.df_top_n.head(5).to_string(index=False))\n",
        "        else:\n",
        "            print(\"No windows met the reporting criteria.\")\n",
        "            \n",
        "        print(f\"\\n[Factor Contribution Stats]\")\n",
        "        if not result.df_phi_summary.empty:\n",
        "            print(result.df_phi_summary.to_string())\n",
        "            \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        logger.info(\"Pipeline execution successful.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline execution failed: {e}\")\n",
        "```\n",
        "\n",
        "<br><br>\n",
        "## **Implemented Callables**"
      ],
      "metadata": {
        "id": "PAuB5wsWiZKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate the methodological configuration dictionary\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Load the `study_parameters` dictionary and verify structural completeness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_config_structure(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the configuration dictionary contains all required top-level keys\n",
        "    and that they are of the correct type (dictionaries).\n",
        "\n",
        "    This implements the structural validation required to ensure the configuration\n",
        "    schema matches the AIMM-X blueprint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master configuration dictionary to validate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if structure is valid. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any required key is missing or is not a dictionary.\n",
        "    \"\"\"\n",
        "    # Define the required top-level keys as specified in the blueprint\n",
        "    required_keys: List[str] = [\n",
        "        \"meta\",\n",
        "        \"trading_calendar\",\n",
        "        \"input_schemas\",\n",
        "        \"attention_processing\",\n",
        "        \"feature_engineering\",\n",
        "        \"deviation_detection\",\n",
        "        \"composite_strength_score\",\n",
        "        \"segmentation_algorithm\",\n",
        "        \"scoring_model\",\n",
        "        \"reporting_and_ranking\",\n",
        "        \"preprint_attention_proxy_reproducibility\"\n",
        "    ]\n",
        "\n",
        "    # Check for missing keys\n",
        "    # Set difference: required - present\n",
        "    missing_keys = set(required_keys) - set(config.keys())\n",
        "\n",
        "    if missing_keys:\n",
        "        error_msg = f\"Configuration is missing required top-level keys: {missing_keys}\"\n",
        "        logger.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Check for type correctness (all top-level values must be dicts)\n",
        "    for key in required_keys:\n",
        "        if not isinstance(config[key], dict):\n",
        "            error_msg = f\"Top-level key '{key}' must be a dictionary, got {type(config[key])}.\"\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(\"Task 1, Step 1: Configuration structural validation passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate numerical parameter ranges and types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_parameter_constraints(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Validates critical nested parameters and cross-field coherence constraints\n",
        "    derived from the AIMM-X methodology (Sections 4.3, 4.4).\n",
        "\n",
        "    Checks:\n",
        "    1. Universe integrity (24 tickers).\n",
        "    2. Hysteresis threshold logic (theta_high > theta_low > 0).\n",
        "    3. Baseline estimation consistency (B == min_periods).\n",
        "    4. Weight positivity for composite score and integrity score.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if all constraints are satisfied. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any methodological constraint is violated.\n",
        "    \"\"\"\n",
        "    # 1. Universe Integrity\n",
        "    # Extract tickers\n",
        "    tickers = config[\"meta\"][\"universe_tickers\"]\n",
        "    if not isinstance(tickers, list):\n",
        "        raise ValueError(\"meta.universe_tickers must be a list.\")\n",
        "\n",
        "    # Constraint: len == 24\n",
        "    if len(tickers) != 24:\n",
        "        raise ValueError(f\"Universe must contain exactly 24 tickers, found {len(tickers)}.\")\n",
        "\n",
        "    # Constraint: Unique and Uppercase\n",
        "    if len(set(tickers)) != len(tickers):\n",
        "        raise ValueError(\"Universe tickers must be unique.\")\n",
        "\n",
        "    for t in tickers:\n",
        "        if not isinstance(t, str) or not t.isupper():\n",
        "            raise ValueError(f\"Ticker '{t}' must be an uppercase string.\")\n",
        "\n",
        "    # 2. Threshold Coherence (Hysteresis)\n",
        "    # Extract parameters\n",
        "    seg_params = config[\"segmentation_algorithm\"][\"parameters\"]\n",
        "    theta_high = seg_params[\"theta_high\"]\n",
        "    theta_low = seg_params[\"theta_low\"]\n",
        "\n",
        "    # Constraint: theta_high > theta_low > 0\n",
        "    if not (isinstance(theta_high, (int, float)) and isinstance(theta_low, (int, float))):\n",
        "         raise ValueError(\"Segmentation thresholds must be numeric.\")\n",
        "\n",
        "    if not (theta_high > theta_low):\n",
        "        raise ValueError(f\"theta_high ({theta_high}) must be strictly greater than theta_low ({theta_low}).\")\n",
        "\n",
        "    if theta_low <= 0:\n",
        "        raise ValueError(f\"theta_low ({theta_low}) must be positive.\")\n",
        "\n",
        "    # 3. Baseline Coherence\n",
        "    # Extract parameters\n",
        "    dev_params = config[\"deviation_detection\"]\n",
        "    B = dev_params[\"baseline_window_B\"]\n",
        "    min_periods = dev_params[\"min_periods\"]\n",
        "    use_lagged = dev_params[\"use_lagged_baseline_only\"]\n",
        "    lag = dev_params[\"lag\"]\n",
        "\n",
        "    # Constraint: B == min_periods (for this specific implementation)\n",
        "    if B != min_periods:\n",
        "        raise ValueError(f\"baseline_window_B ({B}) must equal min_periods ({min_periods}) for consistency.\")\n",
        "\n",
        "    # Constraint: Lagged baseline logic\n",
        "    if use_lagged and lag < 1:\n",
        "        raise ValueError(\"If use_lagged_baseline_only is True, lag must be >= 1.\")\n",
        "\n",
        "    # 4. Weight Positivity\n",
        "    # Composite Score Weights (Eq. 7)\n",
        "    alpha_weights = config[\"composite_strength_score\"][\"weights_alpha\"]\n",
        "    for key, val in alpha_weights.items():\n",
        "        if val < 0:\n",
        "            raise ValueError(f\"Composite weight {key} ({val}) must be non-negative.\")\n",
        "\n",
        "    # Integrity Score Weights (Eq. 14)\n",
        "    omega_weights = config[\"scoring_model\"][\"integrity_score_M\"][\"phi_weights_omega\"]\n",
        "    for key, val in omega_weights.items():\n",
        "        if val < 0:\n",
        "            raise ValueError(f\"Integrity score weight {key} ({val}) must be non-negative.\")\n",
        "\n",
        "    logger.info(\"Task 1, Step 2: Parameter constraint validation passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate string-based model identifiers and create a configuration snapshot.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def check_determinism_fields(config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Checks for unresolved fields that are critical for deterministic reproduction.\n",
        "    Logs warnings or raises errors if critical configuration is missing (None).\n",
        "\n",
        "    Fields checked:\n",
        "    1. trading_calendar.explicit_sessions\n",
        "    2. attention_processing.normalization_definition_for_tilde_a.explicit_formula_or_reference\n",
        "    3. preprint_attention_proxy_reproducibility.random_seed (if synthetic mode)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The master configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if checks complete (warnings may be logged).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If strict mode is enforced (implied for 'cutting-edge exactitude').\n",
        "    \"\"\"\n",
        "    # 1. Explicit Sessions\n",
        "    explicit_sessions = config[\"trading_calendar\"].get(\"explicit_sessions\")\n",
        "    if explicit_sessions is None:\n",
        "        logger.warning(\n",
        "            \"CRITICAL: 'trading_calendar.explicit_sessions' is None. \"\n",
        "            \"Exact reproduction requires a persisted list of session dates. \"\n",
        "            \"The pipeline will attempt to generate this dynamically in Task 4.\"\n",
        "        )\n",
        "    elif not isinstance(explicit_sessions, list):\n",
        "        raise ValueError(\"'explicit_sessions' must be a list of date strings or None.\")\n",
        "\n",
        "    # 2. Normalization Formula\n",
        "    norm_def = config[\"attention_processing\"][\"normalization_definition_for_tilde_a\"]\n",
        "    explicit_formula = norm_def.get(\"explicit_formula_or_reference\")\n",
        "    if explicit_formula is None:\n",
        "        logger.warning(\n",
        "            \"CRITICAL: 'normalization_definition_for_tilde_a.explicit_formula_or_reference' is None. \"\n",
        "            \"Eq. (1) calculation may vary if normalization logic is not explicitly frozen.\"\n",
        "        )\n",
        "\n",
        "    # 3. Random Seed (if synthetic)\n",
        "    proxy_config = config[\"preprint_attention_proxy_reproducibility\"]\n",
        "    if proxy_config.get(\"mode\") == \"synthetic_proxies\":\n",
        "        seed = proxy_config.get(\"random_seed\")\n",
        "        if seed is None:\n",
        "            logger.warning(\n",
        "                \"CRITICAL: Synthetic proxy mode is active but 'random_seed' is None. \"\n",
        "                \"Results will not be deterministic.\"\n",
        "            )\n",
        "\n",
        "    logger.info(\"Task 1, Step 3: Determinism field check completed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_study_config(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the AIMM-X study configuration.\n",
        "\n",
        "    Executes a sequence of validation steps to ensure structural integrity,\n",
        "    parameter coherence, and reproducibility readiness before any data processing begins.\n",
        "\n",
        "    Sequence:\n",
        "    1. validate_config_structure: Checks top-level keys and types.\n",
        "    2. validate_parameter_constraints: Checks numerical logic and universe definitions.\n",
        "    3. check_determinism_fields: Flags missing reproducibility artifacts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The raw configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The validated configuration dictionary (passed through).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any validation step fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 1: Configuration Validation\")\n",
        "\n",
        "    # Step 1: Structure\n",
        "    validate_config_structure(config)\n",
        "\n",
        "    # Step 2: Constraints\n",
        "    validate_parameter_constraints(config)\n",
        "\n",
        "    # Step 3: Determinism\n",
        "    check_determinism_fields(config)\n",
        "\n",
        "    logger.info(\"Task 1 Completed: Configuration is valid.\")\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "73ARYVIKiawX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate the appropriateness of ingested panel data (df_raw_panel)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate the appropriateness of ingested panel data (`df_raw_panel`)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate MultiIndex structure and ordering.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_multiindex_structure(df: pd.DataFrame, config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the DataFrame has a strictly compliant MultiIndex structure:\n",
        "    Levels must be ['date', 'ticker'], unique, and sorted.\n",
        "\n",
        "    This ensures that subsequent vectorized operations (rolling windows, grouping)\n",
        "    perform correctly and deterministically.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary (used for schema verification).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if structure is valid. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If index levels, uniqueness, or sorting constraints are violated.\n",
        "    \"\"\"\n",
        "    # 1. Check Index Type\n",
        "    if not isinstance(df.index, pd.MultiIndex):\n",
        "        raise ValueError(f\"DataFrame must have a MultiIndex, got {type(df.index)}.\")\n",
        "\n",
        "    # 2. Check Level Names and Order\n",
        "    expected_levels = config[\"input_schemas\"][\"panel_index\"][\"index_levels\"]\n",
        "    if df.index.names != expected_levels:\n",
        "        raise ValueError(f\"Index levels must be {expected_levels}, got {df.index.names}.\")\n",
        "\n",
        "    # 3. Check Level Dtypes\n",
        "    # Level 0: date (datetime64[ns])\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index.levels[0]):\n",
        "        raise ValueError(f\"Level 'date' must be datetime64, got {df.index.levels[0].dtype}.\")\n",
        "\n",
        "    # Level 1: ticker (object/string)\n",
        "    if not (pd.api.types.is_object_dtype(df.index.levels[1]) or pd.api.types.is_string_dtype(df.index.levels[1])):\n",
        "        raise ValueError(f\"Level 'ticker' must be object or string, got {df.index.levels[1].dtype}.\")\n",
        "\n",
        "    # 4. Check Uniqueness\n",
        "    if not df.index.is_unique:\n",
        "        duplicates = df[df.index.duplicated()].index.tolist()[:5]\n",
        "        raise ValueError(f\"Index must be unique. Found duplicates: {duplicates}...\")\n",
        "\n",
        "    # 5. Check Sortedness (Monotonicity)\n",
        "    # Let's check if it is sorted according to its structure.\n",
        "    if not df.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"Index must be strictly sorted (monotonic increasing). Please sort before ingestion.\")\n",
        "\n",
        "    logger.info(\"Task 2, Step 1: MultiIndex structure validation passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate presence and dtype of all required columns.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_column_schema(df: pd.DataFrame, config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that all required market microstructure and attention columns are present\n",
        "    and possess compatible data types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if schema is valid. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If columns are missing or have incompatible types.\n",
        "    \"\"\"\n",
        "    # Extract expected columns from config\n",
        "    market_cols = config[\"input_schemas\"][\"market_microstructure_columns\"][\"columns\"]\n",
        "    attention_cols = config[\"input_schemas\"][\"attention_source_columns\"][\"columns\"]\n",
        "\n",
        "    all_required_cols = {**market_cols, **attention_cols}\n",
        "\n",
        "    # 1. Check for Missing Columns\n",
        "    missing_cols = set(all_required_cols.keys()) - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "    # 2. Check Dtypes\n",
        "    for col_name, specs in all_required_cols.items():\n",
        "        actual_dtype = df[col_name].dtype\n",
        "        expected_dtype_desc = specs[\"dtype\"]\n",
        "\n",
        "        # Loose check for float/int compatibility\n",
        "        is_float = pd.api.types.is_float_dtype(actual_dtype)\n",
        "        is_int = pd.api.types.is_integer_dtype(actual_dtype)\n",
        "        is_nullable_int = isinstance(actual_dtype, pd.Int64Dtype)\n",
        "\n",
        "        if \"float\" in expected_dtype_desc and not (is_float or is_int): # Int often acceptable for float cols\n",
        "             raise ValueError(f\"Column '{col_name}' expected float-like, got {actual_dtype}.\")\n",
        "\n",
        "        if \"int\" in expected_dtype_desc and not (is_int or is_nullable_int or is_float): # Float acceptable if holding NaNs\n",
        "             raise ValueError(f\"Column '{col_name}' expected int-like, got {actual_dtype}.\")\n",
        "\n",
        "        # Specific check for nullable attention columns\n",
        "        if col_name in attention_cols:\n",
        "             # Must support NaN (float or Int64)\n",
        "             if not (is_float or is_nullable_int):\n",
        "                 raise ValueError(f\"Attention column '{col_name}' must support NaNs (float or Int64), got {actual_dtype}.\")\n",
        "\n",
        "    logger.info(\"Task 2, Step 2: Column schema validation passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate ticker universe membership and session coverage.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_universe_coverage(df: pd.DataFrame, config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Verifies that the DataFrame contains exactly the tickers specified in the universe\n",
        "    and checks for session coverage consistency if required.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if universe is valid. Raises ValueError otherwise.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If universe set does not match config or if critical session gaps exist.\n",
        "    \"\"\"\n",
        "    # 1. Validate Universe Membership\n",
        "    expected_tickers = set(config[\"meta\"][\"universe_tickers\"])\n",
        "    actual_tickers = set(df.index.get_level_values(\"ticker\").unique())\n",
        "\n",
        "    # Check for missing tickers\n",
        "    missing_tickers = expected_tickers - actual_tickers\n",
        "    if missing_tickers:\n",
        "        raise ValueError(f\"Panel is missing required tickers: {missing_tickers}\")\n",
        "\n",
        "    # Check for extra tickers\n",
        "    extra_tickers = actual_tickers - expected_tickers\n",
        "    if extra_tickers:\n",
        "        raise ValueError(f\"Panel contains extra tickers not in universe: {extra_tickers}\")\n",
        "\n",
        "    # 2. Validate Session Coverage (Optional but recommended)\n",
        "    if config[\"trading_calendar\"].get(\"require_exact_session_coverage_per_ticker\"):\n",
        "        explicit_sessions = config[\"trading_calendar\"].get(\"explicit_sessions\")\n",
        "\n",
        "        if explicit_sessions:\n",
        "            expected_dates = pd.to_datetime(explicit_sessions)\n",
        "            expected_count = len(expected_dates)\n",
        "\n",
        "            # Group by ticker and count dates\n",
        "            # Note: This is a scalar check. Task 4 does the granular date-by-date check.\n",
        "            # Here we just ensure gross counts are reasonable to fail fast.\n",
        "            counts = df.groupby(\"ticker\").size()\n",
        "\n",
        "            mismatched_counts = counts[counts != expected_count]\n",
        "            if not mismatched_counts.empty:\n",
        "                logger.warning(\n",
        "                    f\"Session count mismatch for tickers: {mismatched_counts.to_dict()}. \"\n",
        "                    f\"Expected {expected_count}. Detailed alignment check will occur in Task 4.\"\n",
        "                )\n",
        "                # We don't raise hard error here to allow Task 4 to generate the detailed audit report.\n",
        "\n",
        "    logger.info(\"Task 2, Step 3: Universe coverage validation passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_panel_schema(df_raw_panel: pd.DataFrame, config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the ingested panel data structure.\n",
        "\n",
        "    Ensures the DataFrame is structurally sound, contains all required columns with\n",
        "    correct types, and covers the specified ticker universe before processing begins.\n",
        "\n",
        "    Sequence:\n",
        "    1. validate_multiindex_structure\n",
        "    2. validate_column_schema\n",
        "    3. validate_universe_coverage\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_panel : pd.DataFrame\n",
        "        The raw input DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The validated configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if all validations pass.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If any validation step fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 2: Panel Schema Validation\")\n",
        "\n",
        "    # Step 1: Index Structure\n",
        "    validate_multiindex_structure(df_raw_panel, config)\n",
        "\n",
        "    # Step 2: Columns and Dtypes\n",
        "    validate_column_schema(df_raw_panel, config)\n",
        "\n",
        "    # Step 3: Universe Content\n",
        "    validate_universe_coverage(df_raw_panel, config)\n",
        "\n",
        "    logger.info(\"Task 2 Completed: Panel schema is valid.\")\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "SRNDWaRclp58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Cleanse the panel data (row-level quality enforcement)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse the panel data (row-level quality enforcement)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Enforce OHLC consistency constraints per row.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def identify_ohlc_violations(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Identifies rows where High/Low prices are inconsistent with Open/Close prices.\n",
        "\n",
        "    Logic:\n",
        "    1. High must be >= max(Open, Close)\n",
        "    2. Low must be <= min(Open, Close)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw panel DataFrame containing 'open_price', 'high_price', 'low_price', 'close_price'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A boolean Series (indexed matching df) where True indicates a violation.\n",
        "    \"\"\"\n",
        "    # Extract series for vectorized comparison\n",
        "    # Use .get() or direct access; validation in Task 2 ensures columns exist.\n",
        "    O = df[\"open_price\"]\n",
        "    H = df[\"high_price\"]\n",
        "    L = df[\"low_price\"]\n",
        "    C = df[\"close_price\"]\n",
        "\n",
        "    # Constraint 1: High >= max(Open, Close)\n",
        "    # We use a small epsilon for float comparison if needed, but standard OHLC is usually precise.\n",
        "    # Strict inequality violation: H < max(O, C)\n",
        "    max_oc = np.maximum(O, C)\n",
        "    violation_high = H < max_oc\n",
        "\n",
        "    # Constraint 2: Low <= min(Open, Close)\n",
        "    # Strict inequality violation: L > min(O, C)\n",
        "    min_oc = np.minimum(O, C)\n",
        "    violation_low = L > min_oc\n",
        "\n",
        "    # Combine violations\n",
        "    is_invalid = violation_high | violation_low\n",
        "\n",
        "    # Log statistics\n",
        "    num_violations = is_invalid.sum()\n",
        "    if num_violations > 0:\n",
        "        logger.warning(f\"Task 3, Step 1: Found {num_violations} rows with OHLC inconsistencies.\")\n",
        "\n",
        "    return is_invalid\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Enforce positive price and volume constraints.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def identify_price_volume_violations(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Identifies rows with non-positive Close prices or non-positive Volume.\n",
        "\n",
        "    Logic:\n",
        "    1. Close > 0 (Required for log returns)\n",
        "    2. Volume > 0 (Required for liquid universe assumption)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The raw panel DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        A boolean Series where True indicates a violation.\n",
        "    \"\"\"\n",
        "    C = df[\"close_price\"]\n",
        "    V = df[\"volume\"]\n",
        "\n",
        "    # Constraint 1: Close > 0\n",
        "    # Violation: C <= 0 or NaN\n",
        "    violation_close = (C <= 0) | C.isna()\n",
        "\n",
        "    # Constraint 2: Volume > 0\n",
        "    # Violation: V <= 0 or NaN\n",
        "    violation_volume = (V <= 0) | V.isna()\n",
        "\n",
        "    # Combine violations\n",
        "    is_invalid = violation_close | violation_volume\n",
        "\n",
        "    # Log statistics\n",
        "    num_violations = is_invalid.sum()\n",
        "    if num_violations > 0:\n",
        "        logger.warning(f\"Task 3, Step 2: Found {num_violations} rows with invalid Price/Volume.\")\n",
        "\n",
        "    return is_invalid\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Normalize attention encoding and validate missingness semantics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def enforce_attention_dtypes(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensures attention columns are cast to float64 to support NaN (missing coverage)\n",
        "    vs 0.0 (no activity).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleansed panel DataFrame (valid rows only).\n",
        "    config : Dict[str, Any]\n",
        "        Configuration dictionary defining attention columns.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The DataFrame with attention columns cast to float64.\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    attention_cols = list(config[\"input_schemas\"][\"attention_source_columns\"][\"columns\"].keys())\n",
        "\n",
        "    for col in attention_cols:\n",
        "        if col in df_out.columns:\n",
        "            # Force float64 to handle NaNs and subsequent z-score math\n",
        "            # This preserves NaN as NaN and 0 as 0.0\n",
        "            df_out[col] = df_out[col].astype(\"float64\")\n",
        "\n",
        "    logger.info(\"Task 3, Step 3: Attention columns cast to float64.\")\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def cleanse_panel(df_raw_panel: pd.DataFrame, config: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the row-level cleansing of the panel data.\n",
        "\n",
        "    1. Identifies OHLC inconsistencies.\n",
        "    2. Identifies invalid Price/Volume entries.\n",
        "    3. Quarantines invalid rows.\n",
        "    4. Enforces float dtypes for attention columns in the clean set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_panel : pd.DataFrame\n",
        "        The raw input DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        (df_clean, df_quarantine)\n",
        "        df_clean: The subset of rows passing all QC checks, with correct dtypes.\n",
        "        df_quarantine: The subset of rows that failed QC.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 3: Panel Cleansing\")\n",
        "\n",
        "    # Step 1: OHLC Checks\n",
        "    mask_ohlc = identify_ohlc_violations(df_raw_panel)\n",
        "\n",
        "    # Step 2: Price/Volume Checks\n",
        "    mask_pv = identify_price_volume_violations(df_raw_panel)\n",
        "\n",
        "    # Combine masks (True = Invalid)\n",
        "    mask_quarantine = mask_ohlc | mask_pv\n",
        "\n",
        "    # Split DataFrames\n",
        "    df_quarantine = df_raw_panel[mask_quarantine].copy()\n",
        "    df_clean_raw = df_raw_panel[~mask_quarantine].copy()\n",
        "\n",
        "    # Log results\n",
        "    total_rows = len(df_raw_panel)\n",
        "    quarantined_rows = len(df_quarantine)\n",
        "    clean_rows = len(df_clean_raw)\n",
        "\n",
        "    logger.info(f\"Total rows: {total_rows}\")\n",
        "    logger.info(f\"Quarantined rows: {quarantined_rows} ({quarantined_rows/total_rows:.2%})\")\n",
        "    logger.info(f\"Clean rows: {clean_rows}\")\n",
        "\n",
        "    # Step 3: Enforce Dtypes on Clean Data\n",
        "    df_clean = enforce_attention_dtypes(df_clean_raw, config)\n",
        "\n",
        "    logger.info(\"Task 3 Completed: Panel cleansed.\")\n",
        "    return df_clean, df_quarantine\n"
      ],
      "metadata": {
        "id": "zFGUW7g3mjsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Enforce the canonical trading-session grid\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Enforce the canonical trading-session grid\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Construct or load the canonical session list.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_canonical_sessions(config: Dict[str, Any]) -> pd.DatetimeIndex:\n",
        "    \"\"\"\n",
        "    Retrieves or generates the canonical list of exchange trading sessions.\n",
        "\n",
        "    Priority:\n",
        "    1. Use 'explicit_sessions' from config if present.\n",
        "    2. Generate using 'exchange_calendars' (XNYS) if available.\n",
        "    3. Fallback to pandas USFederalHolidayCalendar (approximation, logged as warning).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DatetimeIndex\n",
        "        The sorted index of valid trading dates.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If sessions cannot be determined.\n",
        "    \"\"\"\n",
        "    # 1. Try explicit list from config\n",
        "    explicit = config[\"trading_calendar\"].get(\"explicit_sessions\")\n",
        "    if explicit is not None:\n",
        "        logger.info(\"Task 4, Step 1: Using explicit session list from config.\")\n",
        "        return pd.to_datetime(explicit).sort_values()\n",
        "\n",
        "    # 2. Try generating via exchange_calendars\n",
        "    start_date = config[\"meta\"][\"date_range\"][\"start\"]\n",
        "    end_date = config[\"meta\"][\"date_range\"][\"end\"]\n",
        "    calendar_name = config[\"trading_calendar\"][\"calendar_name\"]  # e.g., \"XNYS\"\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Task 4, Step 1: Generating {calendar_name} calendar via exchange_calendars.\")\n",
        "        nyse = xcals.get_calendar(calendar_name)\n",
        "        sessions = nyse.sessions_in_range(start_date, end_date)\n",
        "        return sessions\n",
        "    except ImportError:\n",
        "        logger.warning(\"Task 4, Step 1: 'exchange_calendars' not found. Attempting pandas fallback.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Task 4, Step 1: Exchange calendar generation failed: {e}\")\n",
        "\n",
        "    # 3. Fallback: Pandas US Federal Holidays (Approximation for NYSE)\n",
        "    # Note: This might miss special exchange closures (e.g., mourning days), but serves as a robust fallback.\n",
        "    logger.warning(\"Task 4, Step 1: Using Pandas USFederalHolidayCalendar fallback. Verify accuracy for special closures.\")\n",
        "    us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
        "    sessions = pd.date_range(start=start_date, end=end_date, freq=us_bd)\n",
        "    return sessions\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Validate per-ticker alignment to the canonical grid.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def check_ticker_alignment(\n",
        "    df: pd.DataFrame,\n",
        "    canonical_sessions: pd.DatetimeIndex\n",
        ") -> Dict[str, Dict[str, List[pd.Timestamp]]]:\n",
        "    \"\"\"\n",
        "    Checks every ticker in the DataFrame against the canonical session list.\n",
        "    Identifies missing sessions (gaps) and extra sessions (calendar mismatches).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleansed panel DataFrame.\n",
        "    canonical_sessions : pd.DatetimeIndex\n",
        "        The authoritative list of trading dates.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Dict[str, List[pd.Timestamp]]]\n",
        "        A dictionary keyed by ticker, containing 'missing' and 'extra' lists of dates.\n",
        "    \"\"\"\n",
        "    alignment_report = {}\n",
        "    expected_set = set(canonical_sessions)\n",
        "\n",
        "    # Get unique tickers\n",
        "    tickers = df.index.get_level_values(\"ticker\").unique()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        # Extract actual dates for this ticker\n",
        "        # Note: We assume index is (date, ticker) based on Task 2 validation\n",
        "        # Slicing via xs or boolean mask\n",
        "        actual_dates = df.xs(ticker, level=\"ticker\").index\n",
        "        actual_set = set(actual_dates)\n",
        "\n",
        "        # Calculate differences\n",
        "        missing = sorted(list(expected_set - actual_set))\n",
        "        extra = sorted(list(actual_set - expected_set))\n",
        "\n",
        "        if missing or extra:\n",
        "            alignment_report[ticker] = {\n",
        "                \"missing\": missing,\n",
        "                \"extra\": extra\n",
        "            }\n",
        "\n",
        "    return alignment_report\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Produce a session coverage audit report.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def generate_audit_report(\n",
        "    alignment_data: Dict[str, Dict[str, List[pd.Timestamp]]],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Summarizes the alignment check results. Raises error if strict coverage is required\n",
        "    and gaps are found.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alignment_data : Dict\n",
        "        Output from check_ticker_alignment.\n",
        "    config : Dict\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Summary statistics of the audit.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If require_exact_session_coverage_per_ticker is True and mismatches exist.\n",
        "    \"\"\"\n",
        "    total_mismatched_tickers = len(alignment_data)\n",
        "\n",
        "    summary = {\n",
        "        \"status\": \"PASS\" if total_mismatched_tickers == 0 else \"FAIL\",\n",
        "        \"mismatched_tickers_count\": total_mismatched_tickers,\n",
        "        \"details\": {}\n",
        "    }\n",
        "\n",
        "    # Iterate through tickers\n",
        "    for ticker, diffs in alignment_data.items():\n",
        "        n_missing = len(diffs[\"missing\"])\n",
        "        n_extra = len(diffs[\"extra\"])\n",
        "\n",
        "        # Format dates for readability\n",
        "        missing_str = [d.strftime(\"%Y-%m-%d\") for d in diffs[\"missing\"][:5]]\n",
        "        if n_missing > 5: missing_str.append(\"...\")\n",
        "\n",
        "        extra_str = [d.strftime(\"%Y-%m-%d\") for d in diffs[\"extra\"][:5]]\n",
        "        if n_extra > 5: extra_str.append(\"...\")\n",
        "\n",
        "        summary[\"details\"][ticker] = {\n",
        "            \"missing_count\": n_missing,\n",
        "            \"missing_examples\": missing_str,\n",
        "            \"extra_count\": n_extra,\n",
        "            \"extra_examples\": extra_str\n",
        "        }\n",
        "\n",
        "        logger.warning(\n",
        "            f\"Ticker {ticker}: Missing {n_missing} sessions, Extra {n_extra} sessions.\"\n",
        "        )\n",
        "\n",
        "    # Enforce strictness\n",
        "    if config[\"trading_calendar\"][\"require_exact_session_coverage_per_ticker\"]:\n",
        "        if total_mismatched_tickers > 0:\n",
        "            error_msg = (\n",
        "                f\"Strict session coverage required. Found {total_mismatched_tickers} tickers \"\n",
        "                \"with calendar mismatches. See log for details.\"\n",
        "            )\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    logger.info(\"Task 4, Step 3: Audit report generated.\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def enforce_trading_calendar(df_clean: pd.DataFrame, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the trading calendar alignment.\n",
        "\n",
        "    1. Generates/Loads the canonical session list.\n",
        "    2. Checks every ticker for exact alignment.\n",
        "    3. Generates an audit report and enforces strictness if configured.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_clean : pd.DataFrame\n",
        "        The cleansed panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The audit report summary.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 4: Trading Calendar Enforcement\")\n",
        "\n",
        "    # Step 1: Get Canonical Sessions\n",
        "    sessions = get_canonical_sessions(config)\n",
        "    logger.info(f\"Canonical calendar established: {len(sessions)} sessions from {sessions[0].date()} to {sessions[-1].date()}.\")\n",
        "\n",
        "    # Step 2: Check Alignment\n",
        "    alignment_data = check_ticker_alignment(df_clean, sessions)\n",
        "\n",
        "    # Step 3: Audit and Enforce\n",
        "    report = generate_audit_report(alignment_data, config)\n",
        "\n",
        "    logger.info(\"Task 4 Completed: Calendar alignment verified.\")\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "LHWRd704nFAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Prepare per-source attention series for fusion (alignment rules)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Prepare per-source attention series for fusion (alignment rules)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Extract raw attention columns and map to canonical source names.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def extract_raw_attention_series(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Extracts raw attention columns from the cleansed DataFrame and maps them\n",
        "    to their canonical source names (e.g., 'reddit', 'stocktwits') as defined\n",
        "    in the configuration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleansed panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration containing 'source_to_raw_column' mapping.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.Series]\n",
        "        A dictionary where keys are canonical source names and values are\n",
        "        the corresponding raw pandas Series (indexed by date, ticker).\n",
        "    \"\"\"\n",
        "    source_map = config[\"attention_processing\"][\"source_to_raw_column\"]\n",
        "    extracted_series = {}\n",
        "\n",
        "    for canonical_name, raw_col_name in source_map.items():\n",
        "        if raw_col_name not in df.columns:\n",
        "            # This should have been caught in Task 2, but defensive coding is best practice.\n",
        "            raise ValueError(f\"Expected column '{raw_col_name}' for source '{canonical_name}' not found in DataFrame.\")\n",
        "\n",
        "        # Extract and ensure float64 (preserving NaN/0 distinction)\n",
        "        # .copy() is essential to prevent SettingWithCopyWarning during subsequent transformations\n",
        "        series = df[raw_col_name].copy().astype(\"float64\")\n",
        "        extracted_series[canonical_name] = series\n",
        "\n",
        "    logger.info(f\"Task 5, Step 1: Extracted {len(extracted_series)} raw attention series.\")\n",
        "    return extracted_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Apply source-specific resampling rules (within-ticker).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_alignment_rules(\n",
        "    raw_series_dict: Dict[str, pd.Series],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Applies source-specific alignment and filling rules.\n",
        "\n",
        "    Crucially, this handles the 'step-function' nature of some sources (Wikipedia, Trends)\n",
        "    by allowing forward-filling, while preserving the 'event-driven' nature of others\n",
        "    (Reddit, News) by strictly forbidding it.\n",
        "\n",
        "    Logic:\n",
        "    1. Group by ticker (to prevent cross-ticker contamination).\n",
        "    2. If 'ffill_allowed' is True: Apply forward fill.\n",
        "    3. If 'ffill_allowed' is False: Do nothing (keep raw NaNs/0s).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_series_dict : Dict[str, pd.Series]\n",
        "        Dictionary of raw attention series.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration containing 'alignment_rules_per_source'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.Series]\n",
        "        Dictionary of aligned/filled attention series.\n",
        "    \"\"\"\n",
        "    rules = config[\"attention_processing\"][\"alignment_rules_per_source\"]\n",
        "    aligned_series_dict = {}\n",
        "\n",
        "    for source, series in raw_series_dict.items():\n",
        "        rule = rules.get(source)\n",
        "        if not rule:\n",
        "            raise ValueError(f\"No alignment rule defined for source '{source}'.\")\n",
        "\n",
        "        # Group by ticker to ensure operations are isolated per asset\n",
        "        # We assume index level 1 is 'ticker' based on Task 2 validation\n",
        "        grouped = series.groupby(level=\"ticker\", group_keys=False)\n",
        "\n",
        "        if rule[\"ffill_allowed\"]:\n",
        "            # Apply forward fill within each ticker group\n",
        "            # This propagates the last valid observation forward\n",
        "            # Leading NaNs remain NaN (correctly representing 'no coverage yet')\n",
        "            aligned_series = grouped.ffill()\n",
        "            logger.debug(f\"Applied ffill to source '{source}'.\")\n",
        "        else:\n",
        "            # No fill; keep original series\n",
        "            aligned_series = series\n",
        "            logger.debug(f\"No ffill applied to source '{source}'.\")\n",
        "\n",
        "        aligned_series_dict[source] = aligned_series\n",
        "\n",
        "    logger.info(\"Task 5, Step 2: Alignment rules applied.\")\n",
        "    return aligned_series_dict\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Preserve missingness semantics post-alignment.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_missingness_semantics(\n",
        "    aligned_series_dict: Dict[str, pd.Series]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the distinction between 0.0 (zero activity) and NaN (no coverage)\n",
        "    has been preserved after alignment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aligned_series_dict : Dict[str, pd.Series]\n",
        "        The processed attention series.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if semantics appear valid (i.e., series are not empty, dtypes are float).\n",
        "    \"\"\"\n",
        "    for source, series in aligned_series_dict.items():\n",
        "        # Check dtype\n",
        "        if not pd.api.types.is_float_dtype(series.dtype):\n",
        "            raise TypeError(f\"Source '{source}' must be float64, got {series.dtype}.\")\n",
        "\n",
        "        # Log statistics for audit\n",
        "        n_total = len(series)\n",
        "        n_nan = series.isna().sum()\n",
        "        n_zero = (series == 0.0).sum()\n",
        "        n_positive = (series > 0.0).sum()\n",
        "\n",
        "        logger.info(\n",
        "            f\"Source '{source}' Stats: \"\n",
        "            f\"Total={n_total}, NaN={n_nan} ({n_nan/n_total:.1%}), \"\n",
        "            f\"Zero={n_zero} ({n_zero/n_total:.1%}), \"\n",
        "            f\"Pos={n_positive} ({n_positive/n_total:.1%})\"\n",
        "        )\n",
        "\n",
        "        # Sanity check: We shouldn't have lost all data\n",
        "        if n_total > 0 and n_nan == n_total:\n",
        "            logger.warning(f\"Source '{source}' is entirely NaN after alignment. Check input data.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def align_attention_sources(\n",
        "    df_clean: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of attention series for fusion.\n",
        "\n",
        "    1. Extracts raw columns mapped to canonical names.\n",
        "    2. Applies per-source alignment rules (specifically forward-filling where permitted).\n",
        "    3. Validates that missingness semantics (NaN vs 0) are preserved.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_clean : pd.DataFrame\n",
        "        The cleansed panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.Series]\n",
        "        A dictionary of aligned, float64 attention series ready for normalization.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 5: Attention Alignment\")\n",
        "\n",
        "    # Step 1: Extract\n",
        "    raw_series = extract_raw_attention_series(df_clean, config)\n",
        "\n",
        "    # Step 2: Align/Fill\n",
        "    aligned_series = apply_alignment_rules(raw_series, config)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    validate_missingness_semantics(aligned_series)\n",
        "\n",
        "    logger.info(\"Task 5 Completed: Attention sources aligned.\")\n",
        "    return aligned_series\n"
      ],
      "metadata": {
        "id": "MsGgg8WXoRGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Normalize attention sources into (\\tilde{a}_{s,i,t})\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Normalize attention sources into \\(\\tilde{a}_{s,i,t}\\)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1 & 2: Define and apply rolling normalization per source.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_rolling_zscore_normalization(\n",
        "    series: pd.Series,\n",
        "    window: int,\n",
        "    min_periods: int,\n",
        "    epsilon: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the rolling z-score for a single series (grouped by ticker) to serve\n",
        "    as the normalized signal \\(\\tilde{a}_{s,i,t}\\).\n",
        "\n",
        "    Formula:\n",
        "    \\[\n",
        "    \\mu_{t} = \\text{Mean}(x_{t-1} \\dots x_{t-B})\n",
        "    \\sigma_{t} = \\text{Std}(x_{t-1} \\dots x_{t-B})\n",
        "    \\tilde{a}_{t} = \\frac{x_{t} - \\mu_{t}}{\\sigma_{t} + \\epsilon}\n",
        "    \\]\n",
        "\n",
        "    This ensures:\n",
        "    1. No look-ahead bias (baseline uses lagged data).\n",
        "    2. Stationarity (local mean/std adjustment).\n",
        "    3. Comparability across sources (unitless scale).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        Input attention series (must be indexed by date, ticker).\n",
        "    window : int\n",
        "        Rolling window size (B).\n",
        "    min_periods : int\n",
        "        Minimum observations required.\n",
        "    epsilon : float\n",
        "        Small constant to prevent division by zero.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Normalized series \\(\\tilde{a}\\).\n",
        "    \"\"\"\n",
        "    # Group by ticker to isolate assets\n",
        "    # We assume level 1 is ticker based on previous validation\n",
        "    grouped = series.groupby(level=\"ticker\", group_keys=False)\n",
        "\n",
        "    # Compute Rolling Baseline (Lagged)\n",
        "    # shift(1) ensures we use t-1...t-B to calculate stats for t\n",
        "    # This prevents the anomaly at t from polluting the baseline at t\n",
        "    rolled = grouped.shift(1).rolling(window=window, min_periods=min_periods)\n",
        "\n",
        "    mu = rolled.mean()\n",
        "    sigma = rolled.std()\n",
        "\n",
        "    # Compute Z-Score\n",
        "    # Note: x_t is the current value, mu/sigma are from history\n",
        "    z_score = (series - mu) / (sigma + epsilon)\n",
        "\n",
        "    return z_score\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Validate normalized series properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_normalized_series(\n",
        "    normalized_dict: Dict[str, pd.Series]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the normalized series exhibit expected statistical properties\n",
        "    (centered near 0, unit variance) and contain no infinite values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    normalized_dict : Dict[str, pd.Series]\n",
        "        Dictionary of normalized attention series.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    for source, series in normalized_dict.items():\n",
        "        # Check for Inf\n",
        "        if np.isinf(series).any():\n",
        "            n_inf = np.isinf(series).sum()\n",
        "            logger.error(f\"Source '{source}' contains {n_inf} infinite values after normalization.\")\n",
        "\n",
        "            # Replace inf with NaN to prevent downstream crashes, but flag it\n",
        "            # In a strict pipeline, we might raise, but here we patch for robustness\n",
        "            series.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Check stats (ignoring NaNs from warmup)\n",
        "        mean_val = series.mean()\n",
        "        std_val = series.std()\n",
        "\n",
        "        logger.info(\n",
        "            f\"Normalized '{source}': Mean={mean_val:.4f}, Std={std_val:.4f} \"\n",
        "            f\"(Expected ~0.0, ~1.0)\"\n",
        "        )\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def normalize_attention_sources(\n",
        "    aligned_series_dict: Dict[str, pd.Series],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the normalization of attention series.\n",
        "\n",
        "    1. Loads baseline parameters (B, epsilon).\n",
        "    2. Applies lagged rolling z-score normalization to each source.\n",
        "    3. Validates output statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    aligned_series_dict : Dict[str, pd.Series]\n",
        "        Dictionary of aligned attention series.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.Series]\n",
        "        Dictionary of normalized \\(\\tilde{a}_{s,i,t}\\) series.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 6: Attention Normalization\")\n",
        "\n",
        "    # Load parameters\n",
        "    # We use the same window B as the main detection engine for consistency\n",
        "    B = config[\"deviation_detection\"][\"baseline_window_B\"]\n",
        "    min_periods = config[\"deviation_detection\"][\"min_periods\"]\n",
        "    epsilon = config[\"deviation_detection\"][\"epsilon\"]\n",
        "\n",
        "    normalized_dict = {}\n",
        "\n",
        "    for source, series in aligned_series_dict.items():\n",
        "        logger.debug(f\"Normalizing source '{source}' with window={B}...\")\n",
        "\n",
        "        tilde_a = compute_rolling_zscore_normalization(\n",
        "            series,\n",
        "            window=B,\n",
        "            min_periods=min_periods,\n",
        "            epsilon=epsilon\n",
        "        )\n",
        "\n",
        "        normalized_dict[source] = tilde_a\n",
        "\n",
        "    # Validate\n",
        "    validate_normalized_series(normalized_dict)\n",
        "\n",
        "    logger.info(\"Task 6 Completed: Attention sources normalized.\")\n",
        "    return normalized_dict\n"
      ],
      "metadata": {
        "id": "WDPlm1bipCte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Fuse normalized attention into \\(A_{i,t}\\) (Eq. 1)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Fuse normalized attention into \\(A_{i,t}\\) (Eq. 1)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 & 2: Compute weighted sum with strict NaN propagation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_fused_signal(\n",
        "    normalized_dict: Dict[str, pd.Series],\n",
        "    weights: Dict[str, float]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the fused attention signal \\(A_{i,t}\\) using a weighted sum.\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    A_{i,t} = \\sum_{s \\in \\mathcal{S}} w_s \\cdot \\tilde{a}_{s,i,t}\n",
        "    \\]\n",
        "\n",
        "    Constraints:\n",
        "    - Strict NaN propagation: If any source is NaN (missing coverage), the fused\n",
        "      signal is NaN. This prevents partial signals from being misinterpreted as\n",
        "      low attention.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    normalized_dict : Dict[str, pd.Series]\n",
        "        Dictionary of normalized attention series.\n",
        "    weights : Dict[str, float]\n",
        "        Dictionary of fusion weights.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        The fused attention signal \\(A_{i,t}\\), indexed by (date, ticker).\n",
        "    \"\"\"\n",
        "    # 1. Align all series into a DataFrame\n",
        "    # This ensures indices match exactly; missing indices become NaN\n",
        "    df_components = pd.DataFrame(normalized_dict)\n",
        "\n",
        "    # 2. Apply weights\n",
        "    # Multiply each column by its weight\n",
        "    for source, weight in weights.items():\n",
        "        if source in df_components.columns:\n",
        "            df_components[source] *= weight\n",
        "        else:\n",
        "            # Should be caught by validation, but defensive check\n",
        "            raise ValueError(f\"Weight provided for source '{source}' but source series is missing.\")\n",
        "\n",
        "    # 3. Compute Sum with Strict NaN Propagation\n",
        "    # skipna=False ensures that if any component is NaN, the sum is NaN\n",
        "    A_series = df_components.sum(axis=1, skipna=False)\n",
        "\n",
        "    return A_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Validate fused signal properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_fused_signal(A_series: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the fused attention signal.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A_series : pd.Series\n",
        "        The fused signal.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    n_total = len(A_series)\n",
        "    n_nan = A_series.isna().sum()\n",
        "\n",
        "    logger.info(\n",
        "        f\"Fused Signal Stats: Total={n_total}, Valid={n_total - n_nan}, \"\n",
        "        f\"NaN={n_nan} ({n_nan/n_total:.1%})\"\n",
        "    )\n",
        "\n",
        "    if n_total > 0 and n_nan == n_total:\n",
        "        logger.warning(\"Fused signal is entirely NaN. Check source coverage overlap.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def fuse_attention(\n",
        "    normalized_dict: Dict[str, pd.Series],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the fusion of normalized attention signals.\n",
        "\n",
        "    1. Loads fusion weights.\n",
        "    2. Computes weighted sum with strict NaN propagation (Eq. 1).\n",
        "    3. Validates the result.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    normalized_dict : Dict[str, pd.Series]\n",
        "        Dictionary of normalized attention series.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        The fused attention signal \\(A_{i,t}\\).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 7: Attention Fusion\")\n",
        "\n",
        "    # Load weights\n",
        "    weights = config[\"attention_processing\"][\"attention_fusion\"][\"weights_w_s\"]\n",
        "\n",
        "    # Validate weight keys match source keys\n",
        "    source_keys = set(normalized_dict.keys())\n",
        "    weight_keys = set(weights.keys())\n",
        "\n",
        "    if not weight_keys.issubset(source_keys):\n",
        "        missing = weight_keys - source_keys\n",
        "        raise ValueError(f\"Weights defined for missing sources: {missing}\")\n",
        "\n",
        "    # Compute\n",
        "    A_series = compute_fused_signal(normalized_dict, weights)\n",
        "\n",
        "    # Validate\n",
        "    validate_fused_signal(A_series)\n",
        "\n",
        "    logger.info(\"Task 7 Completed: Attention fused.\")\n",
        "    return A_series\n"
      ],
      "metadata": {
        "id": "aM37dx8ip8oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Compute log returns \\(r_{i,t}\\) (Eq. 2)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Compute log returns \\(r_{i,t}\\) (Eq. 2)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1 & 2: Compute log returns per ticker.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_log_returns(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the daily log returns for each ticker.\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    r_{i,t} = \\log\\left(\\frac{C_{i,t}}{C_{i,t-1}}\\right)\n",
        "    \\]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleansed panel DataFrame containing 'close_price'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Log returns indexed by (date, ticker). First observation per ticker is NaN.\n",
        "    \"\"\"\n",
        "    # Extract close prices\n",
        "    # Ensure float64 for precision\n",
        "    close_prices = df[\"close_price\"].astype(\"float64\")\n",
        "\n",
        "    # Group by ticker to isolate assets\n",
        "    # We assume level 1 is ticker based on Task 2 validation\n",
        "    grouped = close_prices.groupby(level=\"ticker\", group_keys=False)\n",
        "\n",
        "    # Compute Log Returns\n",
        "    # r_t = ln(P_t) - ln(P_{t-1})\n",
        "    # This is numerically equivalent to ln(P_t / P_{t-1}) but often more stable\n",
        "    # shift(1) aligns P_{t-1} with P_t\n",
        "    prev_close = grouped.shift(1)\n",
        "\n",
        "    # Calculate\n",
        "    # Note: Task 3 enforced close_price > 0, so log is safe\n",
        "    r_series = np.log(close_prices / prev_close)\n",
        "\n",
        "    return r_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Validate return series properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_returns(r_series: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the computed log returns.\n",
        "\n",
        "    Checks:\n",
        "    1. NaN count matches number of tickers (one lost day per ticker).\n",
        "    2. No infinite values.\n",
        "    3. Flags extreme outliers (> 50% daily move) for audit.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r_series : pd.Series\n",
        "        The log return series.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    # 1. Check NaNs\n",
        "    n_tickers = r_series.index.get_level_values(\"ticker\").nunique()\n",
        "    n_nans = r_series.isna().sum()\n",
        "\n",
        "    # We expect exactly 1 NaN per ticker (the first day)\n",
        "    if n_nans != n_tickers:\n",
        "        logger.debug(\n",
        "            f\"Return NaNs: {n_nans} (Expected {n_tickers} if continuous). \"\n",
        "            \"Discrepancy implies gaps or short history.\"\n",
        "        )\n",
        "\n",
        "    # 2. Check Inf\n",
        "    if np.isinf(r_series).any():\n",
        "        n_inf = np.isinf(r_series).sum()\n",
        "        logger.error(f\"Found {n_inf} infinite returns. Check for zero prices.\")\n",
        "        # Patch infs\n",
        "        r_series.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # 3. Check Outliers\n",
        "    # Log returns > 0.4 (~50%) or < -0.7 (~-50%)\n",
        "    outliers = r_series[np.abs(r_series) > 0.5]\n",
        "    if not outliers.empty:\n",
        "        logger.warning(\n",
        "            f\"Found {len(outliers)} extreme returns (>50% magnitude). \"\n",
        "            f\"Examples: {outliers.head().to_dict()}\"\n",
        "        )\n",
        "\n",
        "    logger.info(\n",
        "        f\"Returns Computed: Mean={r_series.mean():.5f}, Std={r_series.std():.5f}\"\n",
        "    )\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_log_returns(\n",
        "    df_clean: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of log returns.\n",
        "\n",
        "    1. Computes daily log returns per ticker (Eq. 2).\n",
        "    2. Validates the resulting series.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_clean : pd.DataFrame\n",
        "        The cleansed panel DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Log returns \\(r_{i,t}\\).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 8: Log Return Computation\")\n",
        "\n",
        "    # Step 1 & 2: Compute\n",
        "    r_series = calculate_log_returns(df_clean)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    validate_returns(r_series)\n",
        "\n",
        "    logger.info(\"Task 8 Completed: Log returns computed.\")\n",
        "    return r_series\n"
      ],
      "metadata": {
        "id": "PdpM3jC-qtXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Compute volatility proxy \\(\\sigma_{i,t}\\) (Eq. 3)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Compute volatility proxy \\(\\sigma_{i,t}\\) (Eq. 3)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1 & 2: Compute rolling realized volatility.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_realized_volatility(\n",
        "    r_series: pd.Series,\n",
        "    lookback: int,\n",
        "    epsilon: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the rolling realized volatility proxy based on lagged squared returns.\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    \\sigma_{i,t} = \\sqrt{\\frac{1}{L} \\sum_{j=1}^{L} r_{i,t-j}^2 + \\epsilon}\n",
        "    \\]\n",
        "\n",
        "    Key Implementation Details:\n",
        "    - Strictly Lagged: The volatility estimate for time t uses returns from t-1 to t-L.\n",
        "      This ensures no look-ahead bias and that \\(\\sigma_t\\) represents the volatility\n",
        "      regime *entering* the trading session.\n",
        "    - Squared Returns: Uses simple mean of squared returns (RMS) as the estimator.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r_series : pd.Series\n",
        "        Log returns indexed by (date, ticker).\n",
        "    lookback : int\n",
        "        Window size L.\n",
        "    epsilon : float\n",
        "        Small constant for numerical stability.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Volatility proxy \\(\\sigma_{i,t}\\).\n",
        "    \"\"\"\n",
        "    # Group by ticker\n",
        "    grouped = r_series.groupby(level=\"ticker\", group_keys=False)\n",
        "\n",
        "    # 1. Shift by 1 to enforce strict lag (t-1 ... t-L)\n",
        "    lagged_r = grouped.shift(1)\n",
        "\n",
        "    # 2. Square the returns\n",
        "    r_squared = lagged_r.pow(2)\n",
        "\n",
        "    # 3. Compute Rolling Mean of Squared Returns\n",
        "    # min_periods=lookback ensures we don't produce values until full window is available\n",
        "    rolling_variance_proxy = r_squared.rolling(window=lookback, min_periods=lookback).mean()\n",
        "\n",
        "    # 4. Add epsilon and take square root\n",
        "    sigma_series = (rolling_variance_proxy + epsilon).pow(0.5)\n",
        "\n",
        "    return sigma_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Validate volatility series properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_volatility(sigma_series: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the computed volatility proxy.\n",
        "\n",
        "    Checks:\n",
        "    1. Non-negativity.\n",
        "    2. No infinite values.\n",
        "    3. Reasonable magnitude (warn if mean volatility is extremely high/low).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma_series : pd.Series\n",
        "        The volatility series.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    # 1. Check Non-negativity\n",
        "    if (sigma_series < 0).any():\n",
        "        # Should be impossible due to square root\n",
        "        raise ValueError(\"Found negative volatility values. Implementation error.\")\n",
        "\n",
        "    # 2. Check Inf\n",
        "    if np.isinf(sigma_series).any():\n",
        "        n_inf = np.isinf(sigma_series).sum()\n",
        "        logger.error(f\"Found {n_inf} infinite volatility values.\")\n",
        "        sigma_series.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # 3. Check Magnitude\n",
        "    mean_vol = sigma_series.mean()\n",
        "    # Daily vol of 1% (0.01) to 5% (0.05) is typical.\n",
        "    # 0.01 daily ~ 16% annualized. 0.05 daily ~ 80% annualized.\n",
        "    logger.info(f\"Volatility Computed: Mean={mean_vol:.5f}, Std={sigma_series.std():.5f}\")\n",
        "\n",
        "    if mean_vol > 0.10:\n",
        "        logger.warning(f\"Mean daily volatility is very high ({mean_vol:.2%}). Check return scaling.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_volatility(\n",
        "    r_series: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of realized volatility.\n",
        "\n",
        "    1. Loads parameters (L, epsilon).\n",
        "    2. Computes rolling RMS of lagged returns (Eq. 3).\n",
        "    3. Validates the result.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r_series : pd.Series\n",
        "        Log returns.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Volatility proxy \\(\\sigma_{i,t}\\).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 9: Volatility Computation\")\n",
        "\n",
        "    # Load parameters\n",
        "    vol_config = config[\"feature_engineering\"][\"volatility\"]\n",
        "    L = vol_config[\"lookback_L\"]\n",
        "    epsilon = vol_config[\"epsilon\"]\n",
        "\n",
        "    # Compute\n",
        "    sigma_series = calculate_realized_volatility(r_series, L, epsilon)\n",
        "\n",
        "    # Validate\n",
        "    validate_volatility(sigma_series)\n",
        "\n",
        "    logger.info(\"Task 9 Completed: Volatility computed.\")\n",
        "    return sigma_series\n"
      ],
      "metadata": {
        "id": "pg_M38nSFgJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Compute rolling baselines and z-scores (Eq. 4, 5, 6)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Compute rolling baselines and z-scores (Eq. 4, 5, 6)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1 & 2 & 3: Generic Z-Score Computation Logic.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_rolling_zscore(\n",
        "    series: pd.Series,\n",
        "    window: int,\n",
        "    min_periods: int,\n",
        "    epsilon: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the rolling z-score for a generic time series channel.\n",
        "\n",
        "    Equations:\n",
        "    \\[\n",
        "    \\mu_{t} = \\text{Mean}(x_{t-1} \\dots x_{t-B})\n",
        "    \\hat{\\sigma}_{t} = \\sqrt{\\text{Var}(x_{t-1} \\dots x_{t-B}) + \\epsilon}\n",
        "    z_{t} = \\frac{x_{t} - \\mu_{t}}{\\hat{\\sigma}_{t} + \\epsilon}\n",
        "    \\]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        Input series (r, sigma, or A).\n",
        "    window : int\n",
        "        Baseline window B.\n",
        "    min_periods : int\n",
        "        Minimum observations.\n",
        "    epsilon : float\n",
        "        Stability constant.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Z-score series.\n",
        "    \"\"\"\n",
        "    # Group by ticker\n",
        "    grouped = series.groupby(level=\"ticker\", group_keys=False)\n",
        "\n",
        "    # Shift by 1 to enforce strict lag (t-1 ... t-B)\n",
        "    lagged = grouped.shift(1)\n",
        "\n",
        "    # Compute Rolling Stats\n",
        "    # We compute var() to add epsilon inside the sqrt as per Eq. 5\n",
        "    rolled = lagged.rolling(window=window, min_periods=min_periods)\n",
        "\n",
        "    mu = rolled.mean()\n",
        "    variance = rolled.var()\n",
        "\n",
        "    # Eq. 5: Sigma with epsilon inside sqrt\n",
        "    sigma_hat = (variance + epsilon).pow(0.5)\n",
        "\n",
        "    # Eq. 6: Z-score with epsilon in denominator\n",
        "    # Note: x_t is the current value (unshifted)\n",
        "    z_score = (series - mu) / (sigma_hat + epsilon)\n",
        "\n",
        "    return z_score\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_baselines_and_zscores(\n",
        "    r_series: pd.Series,\n",
        "    sigma_series: pd.Series,\n",
        "    A_series: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.Series, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of z-scores for all three channels.\n",
        "\n",
        "    1. Loads baseline parameters.\n",
        "    2. Computes z-scores for Returns, Volatility, and Attention.\n",
        "    3. Validates outputs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    r_series : pd.Series\n",
        "        Log returns.\n",
        "    sigma_series : pd.Series\n",
        "        Volatility proxy.\n",
        "    A_series : pd.Series\n",
        "        Fused attention.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series, pd.Series]\n",
        "        (z_r, z_sigma, z_A)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 10: Baseline & Z-Score Computation\")\n",
        "\n",
        "    # Load parameters\n",
        "    dev_config = config[\"deviation_detection\"]\n",
        "    B = dev_config[\"baseline_window_B\"]\n",
        "    min_periods = dev_config[\"min_periods\"]\n",
        "    epsilon = dev_config[\"epsilon\"]\n",
        "\n",
        "    # Compute Z-Scores\n",
        "    logger.debug(f\"Computing Z-scores with B={B}...\")\n",
        "\n",
        "    z_r = calculate_rolling_zscore(r_series, B, min_periods, epsilon)\n",
        "    z_sigma = calculate_rolling_zscore(sigma_series, B, min_periods, epsilon)\n",
        "    z_A = calculate_rolling_zscore(A_series, B, min_periods, epsilon)\n",
        "\n",
        "    # Basic Validation\n",
        "    for name, z in [(\"Returns\", z_r), (\"Volatility\", z_sigma), (\"Attention\", z_A)]:\n",
        "        n_valid = z.count()\n",
        "        logger.info(f\"{name} Z-Score: {n_valid} valid observations.\")\n",
        "\n",
        "        if np.isinf(z).any():\n",
        "            logger.warning(f\"{name} Z-Score contains infinite values. Replacing with NaN.\")\n",
        "            z.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    logger.info(\"Task 10 Completed: Z-scores computed.\")\n",
        "    return z_r, z_sigma, z_A\n"
      ],
      "metadata": {
        "id": "l65X6LHHyk9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Compute composite strength score \\(s_{i,t}\\) (Eq. 7)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Compute composite strength score \\(s_{i,t}\\) (Eq. 7)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1 & 2: Compute composite strength score.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_composite_score(\n",
        "    z_r: pd.Series,\n",
        "    z_sigma: pd.Series,\n",
        "    z_A: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the composite anomaly strength score \\(s_{i,t}\\).\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    s_{i,t} = \\alpha_r |z_{i,t}^{(r)}| + \\alpha_\\sigma z_{i,t}^{(\\sigma)} + \\alpha_A z_{i,t}^{(A)}\n",
        "    \\]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z_r : pd.Series\n",
        "        Return z-scores.\n",
        "    z_sigma : pd.Series\n",
        "        Volatility z-scores.\n",
        "    z_A : pd.Series\n",
        "        Attention z-scores.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Composite strength score \\(s_{i,t}\\).\n",
        "    \"\"\"\n",
        "    # Load weights\n",
        "    weights = config[\"composite_strength_score\"][\"weights_alpha\"]\n",
        "    alpha_r = weights[\"alpha_r\"]\n",
        "    alpha_sigma = weights[\"alpha_sigma\"]\n",
        "    alpha_A = weights[\"alpha_A\"]\n",
        "\n",
        "    two_sided_r = config[\"composite_strength_score\"][\"return_channel_is_two_sided\"]\n",
        "\n",
        "    # Compute components\n",
        "    # Returns: Absolute value if two-sided (standard for anomaly detection)\n",
        "    term_r = z_r.abs() if two_sided_r else z_r\n",
        "    term_r = term_r * alpha_r\n",
        "\n",
        "    # Volatility: Direct (high vol is anomalous)\n",
        "    term_sigma = z_sigma * alpha_sigma\n",
        "\n",
        "    # Attention: Direct (high attention is anomalous)\n",
        "    term_A = z_A * alpha_A\n",
        "\n",
        "    # Sum\n",
        "    s_series = term_r + term_sigma + term_A\n",
        "\n",
        "    return s_series\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Validate composite score series.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def validate_composite_score(s_series: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the composite strength score.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s_series : pd.Series\n",
        "        The composite score.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    n_valid = s_series.count()\n",
        "\n",
        "    if n_valid == 0:\n",
        "        logger.warning(\"Composite score is empty (all NaN). Check input z-scores.\")\n",
        "        return False\n",
        "\n",
        "    if np.isinf(s_series).any():\n",
        "        n_inf = np.isinf(s_series).sum()\n",
        "        logger.error(f\"Composite score contains {n_inf} infinite values.\")\n",
        "        s_series.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Log distribution stats\n",
        "    logger.info(\n",
        "        f\"Composite Score Stats: Mean={s_series.mean():.4f}, \"\n",
        "        f\"Max={s_series.max():.4f}, Min={s_series.min():.4f}\"\n",
        "    )\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_composite_strength(\n",
        "    z_r: pd.Series,\n",
        "    z_sigma: pd.Series,\n",
        "    z_A: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the composite strength score.\n",
        "\n",
        "    1. Loads weights.\n",
        "    2. Computes weighted sum of z-scores (Eq. 7).\n",
        "    3. Validates result.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    z_r : pd.Series\n",
        "        Return z-scores.\n",
        "    z_sigma : pd.Series\n",
        "        Volatility z-scores.\n",
        "    z_A : pd.Series\n",
        "        Attention z-scores.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Composite strength score \\(s_{i,t}\\).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 11: Composite Score Computation\")\n",
        "\n",
        "    # Compute\n",
        "    s_series = calculate_composite_score(z_r, z_sigma, z_A, config)\n",
        "\n",
        "    # Validate\n",
        "    validate_composite_score(s_series)\n",
        "\n",
        "    logger.info(\"Task 11 Completed: Composite score computed.\")\n",
        "    return s_series\n"
      ],
      "metadata": {
        "id": "ZEPb2etQzbiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Segment suspicious windows using hysteresis (Algorithm 1)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Segment suspicious windows using hysteresis (Algorithm 1)\n",
        "# ==============================================================================\n",
        "\n",
        "class SuspiciousWindow(NamedTuple):\n",
        "    \"\"\"\n",
        "    Represents a contiguous time interval identified as suspicious by the\n",
        "    hysteresis segmentation algorithm.\n",
        "\n",
        "    This structure serves as the fundamental unit of detection in the AIMM-X\n",
        "    pipeline. It captures the temporal extent of an anomaly for a specific asset,\n",
        "    which is subsequently scored and ranked.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ticker : str\n",
        "        The unique identifier of the asset (e.g., \"GME\").\n",
        "    t_start : pd.Timestamp\n",
        "        The timestamp marking the beginning of the suspicious window (inclusive).\n",
        "        Corresponds to the first bar where the composite score exceeded theta_high.\n",
        "    t_end : pd.Timestamp\n",
        "        The timestamp marking the end of the suspicious window (inclusive).\n",
        "        Corresponds to the last bar where the composite score exceeded theta_low\n",
        "        before the exit condition was met.\n",
        "    length_bars : int\n",
        "        The duration of the window in trading sessions (bars).\n",
        "        Calculated as the count of valid trading sessions in [t_start, t_end].\n",
        "    \"\"\"\n",
        "    ticker: str\n",
        "    t_start: pd.Timestamp\n",
        "    t_end: pd.Timestamp\n",
        "    length_bars: int\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1 & 2: Implement the hysteresis state machine.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def apply_hysteresis_logic(\n",
        "    dates: np.ndarray,\n",
        "    scores: np.ndarray,\n",
        "    ticker: str,\n",
        "    theta_high: float,\n",
        "    theta_low: float,\n",
        "    gap_tolerance: int,\n",
        "    min_len: int\n",
        ") -> List[SuspiciousWindow]:\n",
        "    \"\"\"\n",
        "    Applies the Schmitt Trigger (Hysteresis) logic to segment windows for a single ticker.\n",
        "\n",
        "    Algorithm:\n",
        "    1. Scan strictly chronological scores.\n",
        "    2. Trigger 'WINDOW' state if score > theta_high.\n",
        "    3. Maintain 'WINDOW' state if score > theta_low.\n",
        "    4. Allow dips <= theta_low for up to 'gap_tolerance' consecutive bars.\n",
        "    5. Close window if gap limit exceeded or series ends.\n",
        "    6. Filter windows shorter than 'min_len'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dates : np.ndarray\n",
        "        Array of pd.Timestamp.\n",
        "    scores : np.ndarray\n",
        "        Array of float scores corresponding to dates.\n",
        "    ticker : str\n",
        "        Ticker symbol.\n",
        "    theta_high : float\n",
        "        Trigger threshold.\n",
        "    theta_low : float\n",
        "        Sustain threshold.\n",
        "    gap_tolerance : int\n",
        "        Max consecutive bars below theta_low allowed within a window.\n",
        "    min_len : int\n",
        "        Minimum window length (inclusive of start/end).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[SuspiciousWindow]\n",
        "        Detected windows.\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "\n",
        "    # State variables\n",
        "    in_window = False\n",
        "    window_start_idx = -1\n",
        "    last_valid_idx = -1  # Last index where score > theta_low\n",
        "    gap_counter = 0\n",
        "\n",
        "    n = len(scores)\n",
        "\n",
        "    for i in range(n):\n",
        "        s = scores[i]\n",
        "\n",
        "        # Skip NaNs (treat as low score / gap)\n",
        "        if np.isnan(s):\n",
        "            is_high = False\n",
        "            is_low = False\n",
        "        else:\n",
        "            is_high = s > theta_high\n",
        "            is_low = s > theta_low\n",
        "\n",
        "        if not in_window:\n",
        "            # Attempt to start window\n",
        "            if is_high:\n",
        "                in_window = True\n",
        "                window_start_idx = i\n",
        "                last_valid_idx = i\n",
        "                gap_counter = 0\n",
        "        else:\n",
        "            # Already in window\n",
        "            if is_low:\n",
        "                # Signal is strong enough to sustain\n",
        "                last_valid_idx = i\n",
        "                gap_counter = 0  # Reset gap\n",
        "            else:\n",
        "                # Signal dipped\n",
        "                gap_counter += 1\n",
        "\n",
        "                # Check exit condition\n",
        "                if gap_counter > gap_tolerance:\n",
        "                    # Close window at last_valid_idx\n",
        "                    # Length is inclusive: end - start + 1\n",
        "                    # Note: last_valid_idx is relative to the start of this array\n",
        "\n",
        "                    # Check length constraint\n",
        "                    # The window effectively ended at last_valid_idx\n",
        "                    # But we only realize it now (at i)\n",
        "\n",
        "                    # Edge case: if window started but immediately dipped and never recovered,\n",
        "                    # last_valid_idx might be the start_idx.\n",
        "\n",
        "                    win_len = last_valid_idx - window_start_idx + 1\n",
        "\n",
        "                    if win_len >= min_len:\n",
        "                        windows.append(SuspiciousWindow(\n",
        "                            ticker=ticker,\n",
        "                            t_start=dates[window_start_idx],\n",
        "                            t_end=dates[last_valid_idx],\n",
        "                            length_bars=win_len\n",
        "                        ))\n",
        "\n",
        "                    # Reset state\n",
        "                    in_window = False\n",
        "                    gap_counter = 0\n",
        "                    window_start_idx = -1\n",
        "                    last_valid_idx = -1\n",
        "\n",
        "    # Handle window active at end of series\n",
        "    if in_window:\n",
        "        win_len = last_valid_idx - window_start_idx + 1\n",
        "        if win_len >= min_len:\n",
        "            windows.append(SuspiciousWindow(\n",
        "                ticker=ticker,\n",
        "                t_start=dates[window_start_idx],\n",
        "                t_end=dates[last_valid_idx],\n",
        "                length_bars=win_len\n",
        "            ))\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Aggregate windows across all tickers.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def aggregate_windows(\n",
        "    s_series: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies segmentation to all tickers and aggregates results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s_series : pd.Series\n",
        "        Composite strength scores.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame of detected windows.\n",
        "    \"\"\"\n",
        "    # Load params\n",
        "    params = config[\"segmentation_algorithm\"][\"parameters\"]\n",
        "    theta_high = params[\"theta_high\"]\n",
        "    theta_low = params[\"theta_low\"]\n",
        "    gap = params[\"gap_tolerance_g\"]\n",
        "    min_len = params[\"min_window_len_Lmin\"]\n",
        "\n",
        "    all_windows = []\n",
        "\n",
        "    # Iterate by ticker\n",
        "    # Assuming index level 1 is ticker\n",
        "    for ticker, sub_df in s_series.groupby(level=\"ticker\"):\n",
        "        # Extract arrays\n",
        "        # sub_df index is (date, ticker), we need just dates\n",
        "        dates = sub_df.index.get_level_values(\"date\").values\n",
        "        scores = sub_df.values\n",
        "\n",
        "        ticker_windows = apply_hysteresis_logic(\n",
        "            dates, scores, str(ticker),\n",
        "            theta_high, theta_low, gap, min_len\n",
        "        )\n",
        "        all_windows.extend(ticker_windows)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    if not all_windows:\n",
        "        logger.warning(\"No suspicious windows detected.\")\n",
        "        return pd.DataFrame(columns=[\"ticker\", \"t_start\", \"t_end\", \"length_bars\"])\n",
        "\n",
        "    df_windows = pd.DataFrame(all_windows)\n",
        "    return df_windows\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def segment_windows_hysteresis(\n",
        "    s_series: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the segmentation of suspicious windows.\n",
        "\n",
        "    1. Loads segmentation parameters.\n",
        "    2. Applies hysteresis logic per ticker.\n",
        "    3. Aggregates and returns the window list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s_series : pd.Series\n",
        "        Composite strength score.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Detected windows with columns [ticker, t_start, t_end, length_bars].\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 12: Window Segmentation\")\n",
        "\n",
        "    # Aggregate windows\n",
        "    df_windows = aggregate_windows(s_series, config)\n",
        "\n",
        "    logger.info(f\"Task 12 Completed: Detected {len(df_windows)} windows.\")\n",
        "    return df_windows\n"
      ],
      "metadata": {
        "id": "EChHewbKOE27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Compute (\\phi)-signals for each window (Eq. 8–13)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Compute \\(\\phi\\)-signals for each window (Eq. 8–13)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1 & 2: Compute enabled factors (phi_1, phi_2, phi_3, phi_4).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_window_factors(\n",
        "    ticker: str,\n",
        "    t_start: pd.Timestamp,\n",
        "    t_end: pd.Timestamp,\n",
        "    z_r_series: pd.Series,\n",
        "    z_sigma_series: pd.Series,\n",
        "    z_A_series: pd.Series\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes the interpretable evidence factors (phi) for a single window.\n",
        "\n",
        "    Equations:\n",
        "    - phi_1: Sum of squared return z-scores (Shock Intensity).\n",
        "    - phi_2: Sum of positive volatility z-scores (Volatility Anomaly).\n",
        "    - phi_3: Sum of positive attention z-scores (Attention Spike).\n",
        "    - phi_4: Average correlation of Attention with Returns and Volatility (Alignment).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ticker : str\n",
        "        Ticker symbol.\n",
        "    t_start : pd.Timestamp\n",
        "        Window start date.\n",
        "    t_end : pd.Timestamp\n",
        "        Window end date.\n",
        "    z_r_series : pd.Series\n",
        "        Global return z-scores.\n",
        "    z_sigma_series : pd.Series\n",
        "        Global volatility z-scores.\n",
        "    z_A_series : pd.Series\n",
        "        Global attention z-scores.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        Dictionary containing phi_1, phi_2, phi_3, phi_4.\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct slice lookup\n",
        "    try:\n",
        "        # Extract window data\n",
        "        # Note: xs creates a copy, which is fine for small windows\n",
        "        w_z_r = z_r_series.xs(ticker, level=\"ticker\").loc[t_start:t_end]\n",
        "        w_z_sigma = z_sigma_series.xs(ticker, level=\"ticker\").loc[t_start:t_end]\n",
        "        w_z_A = z_A_series.xs(ticker, level=\"ticker\").loc[t_start:t_end]\n",
        "    except KeyError:\n",
        "        # Should not happen if window detection logic is correct\n",
        "        logger.error(f\"Data missing for window {ticker} {t_start}-{t_end}\")\n",
        "        return {\"phi_1\": 0.0, \"phi_2\": 0.0, \"phi_3\": 0.0, \"phi_4\": 0.0}\n",
        "\n",
        "    # Phi 1: Return Shock Intensity (Sum of Squares)\n",
        "    phi_1 = (w_z_r ** 2).sum()\n",
        "\n",
        "    # Phi 2: Volatility Anomaly (Sum of Positive Deviations)\n",
        "    phi_2 = w_z_sigma.clip(lower=0).sum()\n",
        "\n",
        "    # Phi 3: Attention Spike Magnitude (Sum of Positive Deviations)\n",
        "    phi_3 = w_z_A.clip(lower=0).sum()\n",
        "\n",
        "    # Phi 4: Co-movement Alignment (Correlation)\n",
        "    # Handle degenerate cases (len < 2 or constant variance)\n",
        "    if len(w_z_r) < 2:\n",
        "        phi_4 = 0.0\n",
        "    else:\n",
        "        # Compute correlations\n",
        "        # fillna(0) handles cases where std is 0 (constant signal) -> corr is NaN\n",
        "        corr_r_A = w_z_r.corr(w_z_A)\n",
        "        corr_sigma_A = w_z_sigma.corr(w_z_A)\n",
        "\n",
        "        # Treat NaN correlation as 0 (no alignment)\n",
        "        if np.isnan(corr_r_A): corr_r_A = 0.0\n",
        "        if np.isnan(corr_sigma_A): corr_sigma_A = 0.0\n",
        "\n",
        "        phi_4 = 0.5 * (corr_r_A + corr_sigma_A)\n",
        "\n",
        "    return {\n",
        "        \"phi_1\": phi_1,\n",
        "        \"phi_2\": phi_2,\n",
        "        \"phi_3\": phi_3,\n",
        "        \"phi_4\": phi_4\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Set \\(\\phi_5\\) and \\(\\phi_6\\) to zero.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def add_disabled_factors(phi_dict: Dict[str, float]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Adds placeholder values for disabled factors phi_5 and phi_6.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    phi_dict : Dict[str, float]\n",
        "        Dictionary of computed factors.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        Updated dictionary.\n",
        "    \"\"\"\n",
        "    phi_dict[\"phi_5\"] = 0.0\n",
        "    phi_dict[\"phi_6\"] = 0.0\n",
        "    return phi_dict\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_phi_factors(\n",
        "    df_windows: pd.DataFrame,\n",
        "    z_r_series: pd.Series,\n",
        "    z_sigma_series: pd.Series,\n",
        "    z_A_series: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of phi-signals for all detected windows.\n",
        "\n",
        "    1. Iterates through each window in the DataFrame.\n",
        "    2. Slices the underlying z-score series.\n",
        "    3. Computes phi_1 through phi_6.\n",
        "    4. Appends results to the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_windows : pd.DataFrame\n",
        "        DataFrame of detected windows (from Task 12).\n",
        "    z_r_series : pd.Series\n",
        "        Return z-scores.\n",
        "    z_sigma_series : pd.Series\n",
        "        Volatility z-scores.\n",
        "    z_A_series : pd.Series\n",
        "        Attention z-scores.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The input DataFrame enriched with columns 'phi_1' ... 'phi_6'.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 13: Phi Factor Computation\")\n",
        "\n",
        "    if df_windows.empty:\n",
        "        logger.warning(\"No windows to score.\")\n",
        "        # Return empty DF with correct columns\n",
        "        cols = df_windows.columns.tolist() + [f\"phi_{i}\" for i in range(1, 7)]\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    # Pre-sort z-scores to ensure efficient slicing if not already sorted\n",
        "    # (Task 2 validated sorting, but xs() relies on it for performance)\n",
        "    phi_results = []\n",
        "\n",
        "    for idx, row in df_windows.iterrows():\n",
        "        ticker = row[\"ticker\"]\n",
        "        t_start = row[\"t_start\"]\n",
        "        t_end = row[\"t_end\"]\n",
        "\n",
        "        # Compute enabled factors\n",
        "        factors = calculate_window_factors(\n",
        "            ticker, t_start, t_end,\n",
        "            z_r_series, z_sigma_series, z_A_series\n",
        "        )\n",
        "\n",
        "        # Add disabled factors\n",
        "        factors = add_disabled_factors(factors)\n",
        "\n",
        "        phi_results.append(factors)\n",
        "\n",
        "    # Create DataFrame from results\n",
        "    df_phi = pd.DataFrame(phi_results, index=df_windows.index)\n",
        "\n",
        "    # Concatenate with original window info\n",
        "    df_enriched = pd.concat([df_windows, df_phi], axis=1)\n",
        "\n",
        "    logger.info(\"Task 13 Completed: Phi factors computed.\")\n",
        "    return df_enriched\n"
      ],
      "metadata": {
        "id": "-fCc8h60ZmeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Compute Integrity Score \\(M(w)\\) and factor decomposition (Eq. 14)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Compute Integrity Score \\(M(w)\\) and factor decomposition (Eq. 14)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1 & 2 & 3: Compute scores and contributions.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def calculate_scores_and_contributions(\n",
        "    df_windows: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the composite Integrity Score M(w) and individual factor contributions.\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    M(w) = \\sum_{k=1}^{6} \\omega_k \\cdot \\phi_k(w)\n",
        "    \\]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_windows : pd.DataFrame\n",
        "        DataFrame containing windows and phi factors.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame enriched with 'M' and 'contrib_k' columns.\n",
        "    \"\"\"\n",
        "    if df_windows.empty:\n",
        "        # Return empty with correct schema\n",
        "        cols = df_windows.columns.tolist() + [\"M\"] + [f\"contrib_{i}\" for i in range(1, 7)]\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    df_scored = df_windows.copy()\n",
        "\n",
        "    # Load weights\n",
        "    weights = config[\"scoring_model\"][\"integrity_score_M\"][\"phi_weights_omega\"]\n",
        "\n",
        "    # Initialize M\n",
        "    M_series = pd.Series(0.0, index=df_scored.index)\n",
        "\n",
        "    # Compute contributions and sum\n",
        "    for k in range(1, 7):\n",
        "        phi_col = f\"phi_{k}\"\n",
        "        weight_key = f\"omega_{k}\"\n",
        "        contrib_col = f\"contrib_{k}\"\n",
        "\n",
        "        weight = weights.get(weight_key, 0.0)\n",
        "\n",
        "        if phi_col in df_scored.columns:\n",
        "            # Compute contribution: w_k * phi_k\n",
        "            contribution = df_scored[phi_col] * weight\n",
        "            df_scored[contrib_col] = contribution\n",
        "\n",
        "            # Add to total score\n",
        "            M_series += contribution\n",
        "        else:\n",
        "            logger.warning(f\"Phi column '{phi_col}' missing. Treating as 0.\")\n",
        "            df_scored[contrib_col] = 0.0\n",
        "\n",
        "    df_scored[\"M\"] = M_series\n",
        "\n",
        "    return df_scored\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_integrity_scores(\n",
        "    df_windows: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the scoring of suspicious windows.\n",
        "\n",
        "    1. Loads weights.\n",
        "    2. Calculates per-factor contributions.\n",
        "    3. Aggregates into total Integrity Score M.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_windows : pd.DataFrame\n",
        "        DataFrame with phi factors.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Scored windows.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 14: Integrity Scoring\")\n",
        "\n",
        "    # Compute scores and contributions\n",
        "    df_scored = calculate_scores_and_contributions(df_windows, config)\n",
        "\n",
        "    # Log stats\n",
        "    if not df_scored.empty:\n",
        "        logger.info(\n",
        "            f\"Scoring Complete. Max M={df_scored['M'].max():.2f}, \"\n",
        "            f\"Mean M={df_scored['M'].mean():.2f}\"\n",
        "        )\n",
        "\n",
        "    logger.info(\"Task 14 Completed: Windows scored.\")\n",
        "    return df_scored\n"
      ],
      "metadata": {
        "id": "Dc9TwFQi7Y8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Compute rank percentile and apply reporting filters (Eq. 15)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Compute rank percentile and apply reporting filters (Eq. 15)\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Compute rank percentile using Eq. (15).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_rank_percentiles(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the rank percentile for each window based on its Integrity Score M.\n",
        "\n",
        "    Equation:\n",
        "    \\[\n",
        "    \\text{rank\\_pct}(w) = \\frac{\\#\\{w' : M(w') < M(w)\\}}{N_{\\text{windows}}}\n",
        "    \\]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Scored windows DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with 'rank_pct' column added.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        df_out = df.copy()\n",
        "        df_out[\"rank_pct\"] = []\n",
        "        return df_out\n",
        "\n",
        "    # Get ranking parameters\n",
        "    df_out = df.copy()\n",
        "    N = len(df_out)\n",
        "\n",
        "    # Apply ranks\n",
        "    ranks = df_out[\"M\"].rank(method=\"min\")\n",
        "    df_out[\"rank_pct\"] = (ranks - 1) / N\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Apply warmup exclusion filter.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_warmup_mask(\n",
        "    df_windows: pd.DataFrame,\n",
        "    z_series_list: List[pd.Series]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Identifies windows that overlap with the warmup period (where baselines are incomplete).\n",
        "\n",
        "    Logic:\n",
        "    For each ticker, determine the first valid timestamp across all z-score channels.\n",
        "    Any window starting before this timestamp is flagged for exclusion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_windows : pd.DataFrame\n",
        "        The windows DataFrame.\n",
        "    z_series_list : List[pd.Series]\n",
        "        List of z-score series (r, sigma, A).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Boolean mask (True = Keep, False = Exclude).\n",
        "    \"\"\"\n",
        "    if df_windows.empty:\n",
        "        return pd.Series(dtype=bool)\n",
        "\n",
        "    # Determine valid start date per ticker\n",
        "    # We need the latest 'first valid index' across all channels\n",
        "    # (i.e., we need ALL channels to be valid)\n",
        "    ticker_start_dates = {}\n",
        "\n",
        "    # Get unique tickers from windows to optimize\n",
        "    tickers = df_windows[\"ticker\"].unique()\n",
        "\n",
        "    for ticker in tickers:\n",
        "        starts = []\n",
        "        for z_series in z_series_list:\n",
        "            try:\n",
        "                # Slice ticker\n",
        "                ts = z_series.xs(ticker, level=\"ticker\")\n",
        "                first_valid = ts.first_valid_index()\n",
        "                if first_valid:\n",
        "                    starts.append(first_valid)\n",
        "            except KeyError:\n",
        "                pass\n",
        "\n",
        "        if starts:\n",
        "            # We need the latest of the start dates to ensure ALL are valid\n",
        "            ticker_start_dates[ticker] = max(starts)\n",
        "        else:\n",
        "            # No valid data?\n",
        "            ticker_start_dates[ticker] = pd.Timestamp.max\n",
        "\n",
        "    # Apply mask\n",
        "    # Keep if t_start >= valid_start\n",
        "    keep_mask = df_windows.apply(\n",
        "        lambda row: row[\"t_start\"] >= ticker_start_dates.get(row[\"ticker\"], pd.Timestamp.max),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return keep_mask\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Apply z-score artifact exclusion filter.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_artifact_mask(\n",
        "    df_windows: pd.DataFrame,\n",
        "    z_series_list: List[pd.Series],\n",
        "    cutoff: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Identifies windows containing extreme z-score artifacts (e.g., > 20.0).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_windows : pd.DataFrame\n",
        "        The windows DataFrame.\n",
        "    z_series_list : List[pd.Series]\n",
        "        List of z-score series.\n",
        "    cutoff : float\n",
        "        The z-score magnitude threshold.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Boolean mask (True = Keep, False = Exclude).\n",
        "    \"\"\"\n",
        "    if df_windows.empty:\n",
        "        return pd.Series(dtype=bool)\n",
        "\n",
        "    keep_mask = []\n",
        "\n",
        "    # Iterate through each window\n",
        "    for idx, row in df_windows.iterrows():\n",
        "        ticker = row[\"ticker\"]\n",
        "        t_start = row[\"t_start\"]\n",
        "        t_end = row[\"t_end\"]\n",
        "\n",
        "        is_artifact = False\n",
        "\n",
        "        for z_series in z_series_list:\n",
        "            try:\n",
        "                # Slice window\n",
        "                # Use abs() for two-sided check\n",
        "                window_data = z_series.xs(ticker, level=\"ticker\").loc[t_start:t_end]\n",
        "                if (window_data.abs() > cutoff).any():\n",
        "                    is_artifact = True\n",
        "                    break\n",
        "            except KeyError:\n",
        "                pass\n",
        "\n",
        "        keep_mask.append(not is_artifact)\n",
        "\n",
        "    return pd.Series(keep_mask, index=df_windows.index)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def rank_and_filter_windows(\n",
        "    df_scored: pd.DataFrame,\n",
        "    z_r: pd.Series,\n",
        "    z_sigma: pd.Series,\n",
        "    z_A: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates ranking and filtering.\n",
        "\n",
        "    1. Computes rank percentiles on the full set.\n",
        "    2. Applies warmup exclusion.\n",
        "    3. Applies artifact exclusion.\n",
        "    4. Returns both raw (ranked) and filtered datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_scored : pd.DataFrame\n",
        "        Scored windows.\n",
        "    z_r, z_sigma, z_A : pd.Series\n",
        "        Z-score series for filter checks.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, pd.DataFrame]\n",
        "        (df_raw_ranked, df_filtered)\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 15: Ranking and Filtering\")\n",
        "\n",
        "    # 1. Rank\n",
        "    df_raw_ranked = compute_rank_percentiles(df_scored)\n",
        "\n",
        "    # 2. Warmup Filter\n",
        "    filter_config = config[\"reporting_and_ranking\"][\"table_and_artifact_filters\"]\n",
        "\n",
        "    if filter_config[\"exclude_warmup\"]:\n",
        "        mask_warmup = get_warmup_mask(df_raw_ranked, [z_r, z_sigma, z_A])\n",
        "        n_warmup = (~mask_warmup).sum()\n",
        "        logger.info(f\"Warmup Filter: Excluded {n_warmup} windows.\")\n",
        "    else:\n",
        "        mask_warmup = pd.Series(True, index=df_raw_ranked.index)\n",
        "\n",
        "    # 3. Artifact Filter\n",
        "    cutoff = filter_config[\"max_z_score_cutoff\"]\n",
        "    mask_artifact = get_artifact_mask(df_raw_ranked, [z_r, z_sigma, z_A], cutoff)\n",
        "    n_artifact = (~mask_artifact).sum()\n",
        "    logger.info(f\"Artifact Filter (> {cutoff} sigma): Excluded {n_artifact} windows.\")\n",
        "\n",
        "    # 4. Apply Combined Filter\n",
        "    final_mask = mask_warmup & mask_artifact\n",
        "    df_filtered = df_raw_ranked[final_mask].copy()\n",
        "\n",
        "    logger.info(f\"Task 15 Completed: {len(df_filtered)} windows remain after filtering.\")\n",
        "    return df_raw_ranked, df_filtered\n"
      ],
      "metadata": {
        "id": "EpF7IK248gjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Produce study output artifacts\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Produce study output artifacts\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Produce the complete window list (raw, unfiltered).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def prepare_raw_window_list(df_raw_ranked: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares the complete list of detected windows with all scores and factors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_ranked : pd.DataFrame\n",
        "        The ranked windows DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The complete artifact.\n",
        "    \"\"\"\n",
        "    # Ensure column order for readability\n",
        "    base_cols = [\"ticker\", \"t_start\", \"t_end\", \"length_bars\", \"M\", \"rank_pct\"]\n",
        "    phi_cols = [f\"phi_{i}\" for i in range(1, 7)]\n",
        "    contrib_cols = [f\"contrib_{i}\" for i in range(1, 7)]\n",
        "\n",
        "    # Select existing columns (handle case where some might be missing if upstream failed)\n",
        "    cols = [c for c in base_cols + phi_cols + contrib_cols if c in df_raw_ranked.columns]\n",
        "\n",
        "    return df_raw_ranked[cols].copy()\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Produce the Top-N ranked windows table (filtered).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def prepare_top_n_table(\n",
        "    df_filtered: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates the Top-N suspicious windows table for reporting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_filtered : pd.DataFrame\n",
        "        The filtered windows DataFrame.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Top-N windows sorted by Integrity Score M.\n",
        "    \"\"\"\n",
        "    top_n = config[\"reporting_and_ranking\"][\"output_tables\"][\"top_n_windows\"]\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Sort by M descending\n",
        "    df_sorted = df_filtered.sort_values(\"M\", ascending=False)\n",
        "\n",
        "    # Take top N\n",
        "    df_top = df_sorted.head(top_n)\n",
        "\n",
        "    # Select reporting columns\n",
        "    cols = [\"ticker\", \"t_start\", \"t_end\", \"M\", \"rank_pct\"]\n",
        "    return df_top[cols].copy()\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Produce factor contribution summary statistics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def compute_phi_statistics(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes summary statistics for each phi factor across all detected windows.\n",
        "\n",
        "    Metrics: Mean, Median, Max, Std, Nonzero Percentage.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw : pd.DataFrame\n",
        "        The raw windows DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Summary statistics table.\n",
        "    \"\"\"\n",
        "    if df_raw.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    stats_list = []\n",
        "\n",
        "    # iterate through the range (1, 7)\n",
        "    for i in range(1, 7):\n",
        "        col = f\"phi_{i}\"\n",
        "        if col in df_raw.columns:\n",
        "            series = df_raw[col]\n",
        "            stats = {\n",
        "                \"factor\": col,\n",
        "                \"mean\": series.mean(),\n",
        "                \"median\": series.median(),\n",
        "                \"max\": series.max(),\n",
        "                \"std\": series.std(),\n",
        "                \"nonzero_pct\": (series > 0).mean()\n",
        "            }\n",
        "            stats_list.append(stats)\n",
        "\n",
        "    return pd.DataFrame(stats_list).set_index(\"factor\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def export_artifacts(\n",
        "    df_raw_ranked: pd.DataFrame,\n",
        "    df_filtered: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation of final study artifacts.\n",
        "\n",
        "    1. Prepares the full raw window list.\n",
        "    2. Generates the Top-N filtered table.\n",
        "    3. Computes factor summary statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_ranked : pd.DataFrame\n",
        "        All detected windows, ranked.\n",
        "    df_filtered : pd.DataFrame\n",
        "        Windows passing quality filters.\n",
        "    config : Dict[str, Any]\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        Dictionary containing 'raw_windows', 'top_n', and 'phi_stats'.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Task 16: Artifact Export\")\n",
        "\n",
        "    artifacts = {}\n",
        "\n",
        "    # 1. Raw List\n",
        "    artifacts[\"raw_windows\"] = prepare_raw_window_list(df_raw_ranked)\n",
        "\n",
        "    # 2. Top N\n",
        "    artifacts[\"top_n\"] = prepare_top_n_table(df_filtered, config)\n",
        "\n",
        "    # 3. Stats\n",
        "    artifacts[\"phi_stats\"] = compute_phi_statistics(df_raw_ranked)\n",
        "\n",
        "    logger.info(\"Task 16 Completed: Artifacts generated.\")\n",
        "    return artifacts\n"
      ],
      "metadata": {
        "id": "pniJqLZ98i79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Build the end-to-end orchestrator callable\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Build the end-to-end orchestrator callable\n",
        "# ==============================================================================\n",
        "\n",
        "class PipelineResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    A comprehensive container for all artifacts, data structures, and audit logs\n",
        "    produced by the execution of the AIMM-X Market Integrity Monitoring pipeline.\n",
        "\n",
        "    This object encapsulates the complete state of a study run, ensuring\n",
        "    reproducibility and facilitating downstream analysis or reporting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config_snapshot : Dict[str, Any]\n",
        "        The fully validated configuration dictionary used for this run.\n",
        "        Includes resolved parameters (e.g., explicit session lists) to ensure\n",
        "        deterministic reproduction.\n",
        "\n",
        "    audit_log : Dict[str, Any]\n",
        "        A structured log containing execution metadata, including:\n",
        "        - Library versions (pandas, numpy).\n",
        "        - Step-by-step completion status.\n",
        "        - Data quality metrics (quarantined rows, missing sessions).\n",
        "        - Counts of detected and filtered windows.\n",
        "\n",
        "    df_windows_raw : pd.DataFrame\n",
        "        The complete set of detected windows prior to any filtering.\n",
        "        Contains all scoring factors (phi_1...phi_6), contributions, and\n",
        "        integrity scores (M). Useful for analyzing the distribution of anomalies.\n",
        "\n",
        "    df_windows_filtered : pd.DataFrame\n",
        "        The subset of windows that passed all quality filters (e.g., warmup\n",
        "        exclusion, artifact removal). This dataset represents the valid\n",
        "        candidates for analyst review.\n",
        "\n",
        "    df_top_n : pd.DataFrame\n",
        "        A curated table of the highest-ranking windows (by Integrity Score M),\n",
        "        formatted for reporting (e.g., the \"Top 15\" table in the paper).\n",
        "\n",
        "    df_phi_summary : pd.DataFrame\n",
        "        Summary statistics (mean, median, max, nonzero%) for each phi-factor\n",
        "        across the detected windows. Used to diagnose factor dominance or\n",
        "        scaling issues.\n",
        "\n",
        "    intermediate_series : Dict[str, pd.Series]\n",
        "        A dictionary containing the intermediate time-series computed during\n",
        "        the pipeline, including:\n",
        "        - 'r_series': Log returns.\n",
        "        - 'sigma_series': Volatility proxy.\n",
        "        - 'A_series': Fused attention signal.\n",
        "        - 's_series': Composite strength score.\n",
        "        - 'z_r', 'z_sigma', 'z_A': Individual channel z-scores.\n",
        "        Retained for debugging, visualization, and deep-dive analysis.\n",
        "    \"\"\"\n",
        "    config_snapshot: Dict[str, Any]\n",
        "    audit_log: Dict[str, Any]\n",
        "    df_windows_raw: pd.DataFrame\n",
        "    df_windows_filtered: pd.DataFrame\n",
        "    df_top_n: pd.DataFrame\n",
        "    df_phi_summary: pd.DataFrame\n",
        "    intermediate_series: Dict[str, pd.Series]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1 & 2 & 3: Implement the orchestrator with audit logging.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "def run_aimm_x_pipeline(\n",
        "    df_raw_panel: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> PipelineResult:\n",
        "    \"\"\"\n",
        "    Executes the complete AIMM-X Market Integrity Monitoring pipeline.\n",
        "\n",
        "    Sequence:\n",
        "    1.  Validate Config (Task 1)\n",
        "    2.  Validate Panel Schema (Task 2)\n",
        "    3.  Cleanse Panel (Task 3)\n",
        "    4.  Enforce Calendar (Task 4)\n",
        "    5.  Align Attention (Task 5)\n",
        "    6.  Normalize Attention (Task 6)\n",
        "    7.  Fuse Attention (Task 7)\n",
        "    8.  Compute Returns (Task 8)\n",
        "    9.  Compute Volatility (Task 9)\n",
        "    10. Compute Z-Scores (Task 10)\n",
        "    11. Compute Composite Score (Task 11)\n",
        "    12. Segment Windows (Task 12)\n",
        "    13. Compute Phi Factors (Task 13)\n",
        "    14. Score Windows (Task 14)\n",
        "    15. Rank & Filter (Task 15)\n",
        "    16. Export Artifacts (Task 16)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_raw_panel : pd.DataFrame\n",
        "        The raw input panel data.\n",
        "    config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    PipelineResult\n",
        "        A named tuple containing all study artifacts and audit logs.\n",
        "    \"\"\"\n",
        "    audit_log = {\n",
        "        \"python_version\": sys.version,\n",
        "        \"pandas_version\": pd.__version__,\n",
        "        \"numpy_version\": np.__version__,\n",
        "        \"steps_completed\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Task 1: Validate Config\n",
        "        logger.info(\"=== Pipeline Step 1: Config Validation ===\")\n",
        "        validated_config = validate_study_config(config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 1\")\n",
        "\n",
        "        # Task 2: Validate Schema\n",
        "        logger.info(\"=== Pipeline Step 2: Schema Validation ===\")\n",
        "        validate_panel_schema(df_raw_panel, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 2\")\n",
        "\n",
        "        # Task 3: Cleanse Panel\n",
        "        logger.info(\"=== Pipeline Step 3: Panel Cleansing ===\")\n",
        "        df_clean, df_quarantine = cleanse_panel(df_raw_panel, validated_config)\n",
        "        audit_log[\"quarantined_rows\"] = len(df_quarantine)\n",
        "        audit_log[\"clean_rows\"] = len(df_clean)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 3\")\n",
        "\n",
        "        # Task 4: Enforce Calendar\n",
        "        logger.info(\"=== Pipeline Step 4: Calendar Enforcement ===\")\n",
        "        calendar_report = enforce_trading_calendar(df_clean, validated_config)\n",
        "        audit_log[\"calendar_report\"] = calendar_report\n",
        "        audit_log[\"steps_completed\"].append(\"Task 4\")\n",
        "\n",
        "        # Task 5: Align Attention\n",
        "        logger.info(\"=== Pipeline Step 5: Attention Alignment ===\")\n",
        "        aligned_attention = align_attention_sources(df_clean, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 5\")\n",
        "\n",
        "        # Task 6: Normalize Attention\n",
        "        logger.info(\"=== Pipeline Step 6: Attention Normalization ===\")\n",
        "        tilde_a = normalize_attention_sources(aligned_attention, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 6\")\n",
        "\n",
        "        # Task 7: Fuse Attention\n",
        "        logger.info(\"=== Pipeline Step 7: Attention Fusion ===\")\n",
        "        A_series = fuse_attention(tilde_a, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 7\")\n",
        "\n",
        "        # Task 8: Compute Returns\n",
        "        logger.info(\"=== Pipeline Step 8: Return Computation ===\")\n",
        "        r_series = compute_log_returns(df_clean, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 8\")\n",
        "\n",
        "        # Task 9: Compute Volatility\n",
        "        logger.info(\"=== Pipeline Step 9: Volatility Computation ===\")\n",
        "        sigma_series = compute_volatility(r_series, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 9\")\n",
        "\n",
        "        # Task 10: Compute Z-Scores\n",
        "        logger.info(\"=== Pipeline Step 10: Z-Score Computation ===\")\n",
        "        z_r, z_sigma, z_A = compute_baselines_and_zscores(\n",
        "            r_series, sigma_series, A_series, validated_config\n",
        "        )\n",
        "        audit_log[\"steps_completed\"].append(\"Task 10\")\n",
        "\n",
        "        # Task 11: Composite Score\n",
        "        logger.info(\"=== Pipeline Step 11: Composite Score ===\")\n",
        "        s_series = compute_composite_strength(z_r, z_sigma, z_A, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 11\")\n",
        "\n",
        "        # Task 12: Segment Windows\n",
        "        logger.info(\"=== Pipeline Step 12: Window Segmentation ===\")\n",
        "        df_windows_raw = segment_windows_hysteresis(s_series, validated_config)\n",
        "        audit_log[\"raw_windows_count\"] = len(df_windows_raw)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 12\")\n",
        "\n",
        "        # Task 13: Compute Phi Factors\n",
        "        logger.info(\"=== Pipeline Step 13: Phi Factor Computation ===\")\n",
        "        df_phi = compute_phi_factors(\n",
        "            df_windows_raw, z_r, z_sigma, z_A, validated_config\n",
        "        )\n",
        "        audit_log[\"steps_completed\"].append(\"Task 13\")\n",
        "\n",
        "        # Task 14: Score Windows\n",
        "        logger.info(\"=== Pipeline Step 14: Scoring ===\")\n",
        "        df_scored = compute_integrity_scores(df_phi, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 14\")\n",
        "\n",
        "        # Task 15: Rank & Filter\n",
        "        logger.info(\"=== Pipeline Step 15: Ranking & Filtering ===\")\n",
        "        df_ranked, df_filtered = rank_and_filter_windows(\n",
        "            df_scored, z_r, z_sigma, z_A, validated_config\n",
        "        )\n",
        "        audit_log[\"filtered_windows_count\"] = len(df_filtered)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 15\")\n",
        "\n",
        "        # Task 16: Export Artifacts\n",
        "        logger.info(\"=== Pipeline Step 16: Artifact Export ===\")\n",
        "        artifacts = export_artifacts(df_ranked, df_filtered, validated_config)\n",
        "        audit_log[\"steps_completed\"].append(\"Task 16\")\n",
        "\n",
        "        # Package Results\n",
        "        intermediate_series = {\n",
        "            \"r_series\": r_series,\n",
        "            \"sigma_series\": sigma_series,\n",
        "            \"A_series\": A_series,\n",
        "            \"s_series\": s_series,\n",
        "            \"z_r\": z_r,\n",
        "            \"z_sigma\": z_sigma,\n",
        "            \"z_A\": z_A\n",
        "        }\n",
        "\n",
        "        return PipelineResult(\n",
        "            config_snapshot=validated_config,\n",
        "            audit_log=audit_log,\n",
        "            df_windows_raw=artifacts[\"raw_windows\"],\n",
        "            df_windows_filtered=df_filtered,\n",
        "            df_top_n=artifacts[\"top_n\"],\n",
        "            df_phi_summary=artifacts[\"phi_stats\"],\n",
        "            intermediate_series=intermediate_series\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline failed at step: {audit_log['steps_completed'][-1] if audit_log['steps_completed'] else 'Start'}\")\n",
        "        logger.critical(f\"Error: {str(e)}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "JRVOsFr9XkIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}